{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50m170L8kmQC",
    "outputId": "c817cc70-c5a9-4b85-8baa-c45186e45d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting matchms\n",
      "  Downloading matchms-0.18.0-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from matchms) (3.7.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from matchms) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from matchms) (4.65.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from matchms) (3.1)\n",
      "Requirement already satisfied: numba>=0.47 in /usr/local/lib/python3.10/dist-packages (from matchms) (0.56.4)\n",
      "Collecting pyteomics>=4.2\n",
      "  Downloading pyteomics-4.6-py2.py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.1/235.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from matchms) (2.27.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from matchms) (1.22.4)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from matchms) (4.9.2)\n",
      "Collecting pickydict>=0.4.0\n",
      "  Downloading pickydict-0.4.0-py3-none-any.whl (6.1 kB)\n",
      "Collecting sparsestack>=0.4.1\n",
      "  Downloading sparsestack-0.4.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.47->matchms) (67.7.2)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.47->matchms) (0.39.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->matchms) (1.14.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (4.39.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->matchms) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->matchms) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->matchms) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->matchms) (1.26.15)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->matchms) (1.16.0)\n",
      "Installing collected packages: pyteomics, pickydict, deprecated, sparsestack, matchms\n",
      "Successfully installed deprecated-1.2.13 matchms-0.18.0 pickydict-0.4.0 pyteomics-4.6 sparsestack-0.4.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting rdkit\n",
      "  Downloading rdkit-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (8.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n",
      "Installing collected packages: rdkit\n",
      "Successfully installed rdkit-2022.9.5\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
      "Building wheels for collected packages: torch_geometric\n",
      "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910476 sha256=b7ed7c647db1c98e56a950593f60db8aae2fd5b4b3b389d617505e8470766315\n",
      "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
      "Successfully built torch_geometric\n",
      "Installing collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.3.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=256439 sha256=65da3e9f07cfd3b25cdb51e104c7fc413fa827854c9ba306fd5a2f646beea513\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install matchms\n",
    "!pip install rdkit\n",
    "!pip install torch_geometric\n",
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDErmuSHlP-z"
   },
   "outputs": [],
   "source": [
    "from matchms.importing import load_from_msp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import matchms\n",
    "from matchms import Spectrum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool, GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3oU8Vn3lQBM",
    "outputId": "e4ac4b61-a358-4272-caae-3bb06b16358a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbbhD8VUlQD2"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/MyDrive/NIST_SMALL\")\n",
    "BASE_DIRECTORY = \"/content/drive/MyDrive/NIST_SMALL\"\n",
    "\n",
    "TEST_DATA_SIZE = 5000\n",
    "OUTPUT_SIZE = 1000\n",
    "INTENSITY_POWER = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQHXkp8olQG9"
   },
   "outputs": [],
   "source": [
    "# with open(\"/content/drive/MyDrive/NIST_SMALL/Preprocessed_test_log_preparation_no_sparse_small.output\", 'rb') as handle:\n",
    "#    data_list_test  = pickle.load(handle)\n",
    "\n",
    "# with open(\"/content/drive/MyDrive/NIST_SMALL/Preprocessed_train_log_preparation_no_sparse_small.output\", 'rb') as handle:\n",
    "#    data_list_train  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ien3jRsYjaY8"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/NIST_SMALL/Preprocessed_test_pow_preparation_no_sparse_small.output\", 'rb') as handle:\n",
    "    data_list_test  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygTxmHh0juJv"
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_VALIDATION = 5000\n",
    "# shuffled_indices = np.random.permutation(len(data_list_train))\n",
    "# train_indices = shuffled_indices[NUMBER_OF_VALIDATION:]\n",
    "# val_indices = shuffled_indices[:NUMBER_OF_VALIDATION]\n",
    "\n",
    "test_dataset = data_list_test[:NUMBER_OF_VALIDATION]\n",
    "\n",
    "# validation_dataset = torch.utils.data.Subset(data_list_train, val_indices)\n",
    "# train_dataset = torch.utils.data.Subset(data_list_train, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ued52HQujwcq"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/NIST_SMALL/train_subset_pow.pkl\", 'rb') as handle:\n",
    "    train_dataset  = pickle.load(handle)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/NIST_SMALL/validation_subset_pow.pkl\", 'rb') as handle:\n",
    "    validation_dataset  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US04fy7IlQLM"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 2000\n",
    "NODE_FEATURES = 50\n",
    "EDGE_EMBEDDING = 10\n",
    "MASS_SHIFT = 5 \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NO7eAbp0lQNe"
   },
   "outputs": [],
   "source": [
    "def mask_prediction_by_mass(total_mass, raw_prediction, index_shift):\n",
    "    # Zero out predictions to the right of the maximum possible mass.\n",
    "    # input \n",
    "    # anchor_indices: shape (,batch_size) = ex [3,4,5]\n",
    "    #     total_mass = Weights of whole molecule, not only fragment\n",
    "    # data: shape (batch_size, embedding), embedding from GNN in our case\n",
    "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
    "    # \n",
    "\n",
    "    data = raw_prediction.type(torch.float64)\n",
    "    \n",
    "    total_mass = torch.round(total_mass).type(torch.int64)\n",
    "    indices = torch.arange(data.shape[-1])[None, ...].to(device)\n",
    "\n",
    "    right_of_total_mass = indices > (\n",
    "            total_mass[..., None] +\n",
    "            index_shift)\n",
    "    return torch.where(right_of_total_mass, torch.zeros_like(data),\n",
    "                        data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4N4vUlglQQw"
   },
   "outputs": [],
   "source": [
    "def scatter_by_anchor_indices(anchor_indices, data, index_shift):\n",
    "    # reverse vector by anchor_indices and rest set to zero\n",
    "    # input \n",
    "    # anchor_indices: shape (,batch_size) = ex [3,4,5]\n",
    "    #     total_mass = Weights of whole molecule, not only fragment\n",
    "    # data: shape (batch_size, embedding), embedding from GNN in our case\n",
    "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
    "    \n",
    "    index_shift = index_shift\n",
    "    anchor_indices = anchor_indices\n",
    "    data = data.type(torch.float64)\n",
    "    batch_size = data.shape[0]\n",
    "    \n",
    "    num_data_columns = data.shape[-1]\n",
    "    indices = torch.arange(num_data_columns)[None, ...].to(device)\n",
    "    shifted_indices = anchor_indices[..., None] - indices + index_shift\n",
    "    valid_indices = shifted_indices >= 0\n",
    "\n",
    "   \n",
    "\n",
    "    batch_indices = torch.tile(\n",
    "          torch.arange(batch_size)[..., None], [1, num_data_columns]).to(device)\n",
    "    shifted_indices += batch_indices * num_data_columns\n",
    "\n",
    "    shifted_indices = torch.reshape(shifted_indices, [-1])\n",
    "    num_elements = data.shape[0] * data.shape[1]\n",
    "    row_indices = torch.arange(num_elements).to(device)\n",
    "    stacked_indices = torch.stack([row_indices, shifted_indices], axis=1)\n",
    "\n",
    "\n",
    "    lower_batch_boundaries = torch.reshape(batch_indices * num_data_columns, [-1])\n",
    "    upper_batch_boundaries = torch.reshape(((batch_indices + 1) * num_data_columns),\n",
    "                                          [-1])\n",
    "\n",
    "    valid_indices = torch.logical_and(shifted_indices >= lower_batch_boundaries,\n",
    "                                     shifted_indices < upper_batch_boundaries)\n",
    "\n",
    "    stacked_indices = stacked_indices[valid_indices]\n",
    "    \n",
    "\n",
    "    dense_shape = torch.tile(torch.tensor(num_elements)[..., None], [2]).type(torch.int32)\n",
    "\n",
    "    scattering_matrix = torch.sparse.FloatTensor(stacked_indices.type(torch.int64).T,\n",
    "                                                 torch.ones_like(stacked_indices[:, 0]).type(torch.float64),\n",
    "                                                dense_shape.tolist())\n",
    "\n",
    "    flattened_data = torch.reshape(data, [-1])[..., None]\n",
    "    flattened_output = torch.sparse.mm(scattering_matrix, flattened_data)\n",
    "    return torch.reshape(torch.transpose(flattened_output, 0, 1), [-1, num_data_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dln8v3AClQSS"
   },
   "outputs": [],
   "source": [
    "def reverse_prediction(total_mass, raw_prediction, index_shift):\n",
    "    # reverse vector by anchor_indices and rest set to zero and make preproessing\n",
    "    # input \n",
    "    # total_mass: shape (,batch_size) = ex [3,4,5]\n",
    "    #     total_mass = Weights of whole molecule, not only fragment\n",
    "    # raw_prediction: shape (batch_size, embedding), embedding from GNN in our case\n",
    "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
    "    #     total_mass = feature_dict[fmap_constants.MOLECULE_WEIGHT][..., 0]\n",
    "    \n",
    "    total_mass = torch.round(total_mass).type(torch.int32)\n",
    "    return scatter_by_anchor_indices(\n",
    "        total_mass, raw_prediction, index_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBA9SAJolQUN"
   },
   "outputs": [],
   "source": [
    "def dot_product(true, pred, mass_pow=3, intensity_pow=0.6):\n",
    "    # shape for true and pred is one dimensional array\n",
    "    # pred (number_of_predicted_bins)\n",
    "    # defaul value for mass_pow and intensity_pow is set for Stein dot product\n",
    "    assert true.ndim == pred.ndim and true.ndim == 1\n",
    "    length = true.shape[-1]\n",
    "    mass = np.arange(length).astype(np.float64)\n",
    "        \n",
    "    wl = mass ** mass_pow * pred**intensity_pow\n",
    "    wu = mass ** mass_pow * true**intensity_pow\n",
    "    \n",
    "    pred_weighted_norm = np.sqrt(np.sum((wl**2)))\n",
    "    true_weighted_norm = np.sqrt(np.sum((wu**2)))\n",
    "    \n",
    "    result = np.sum(wl*wu) / (pred_weighted_norm * true_weighted_norm)\n",
    "    \n",
    "    if np.isnan(result):\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsDAQUwQlQWk"
   },
   "outputs": [],
   "source": [
    "def validate_similarities(true, pred, mass_pow, intensity_pow):\n",
    "    # Helper function for validation\n",
    "    similarities = np.array([])\n",
    "    for true_instance, pred_instance in zip(true, pred):\n",
    "        tmp = dot_product(true_instance, pred_instance, mass_pow=mass_pow, intensity_pow=intensity_pow)\n",
    "        \n",
    "        similarities = np.concatenate((similarities, tmp), axis=None)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uez-8A76lQa_"
   },
   "outputs": [],
   "source": [
    "class SKIPblock(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, USE_dropout=True, dropout_rate = 0.2):\n",
    "        super().__init__()\n",
    "        #only need to change shape of the residual if num_channels changes (i.e. in_c != out_c)\n",
    "        #[bs,in_c,seq_length]->conv(1,in_c,out_c)->[bs,out_c,seq_length]\n",
    "        \n",
    "        self.hidden1= nn.utils.weight_norm(nn.Linear(in_features, hidden_features),name='weight',dim=0)\n",
    "        if USE_dropout:\n",
    "            self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.hidden2 = nn.utils.weight_norm(nn.Linear(hidden_features, in_features),name='weight',dim=0)\n",
    "        if USE_dropout:\n",
    "            self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden = self.hidden1(x)\n",
    "        hidden = self.dropout1(hidden)\n",
    "        hidden = self.relu1(hidden)\n",
    "\n",
    "        hidden = self.hidden2(hidden)\n",
    "        hidden = hidden + x\n",
    "        hidden = self.relu2(hidden)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2dKIsoglQcs",
    "outputId": "817d923a-2de7-4a25-c5ee-54485aa740c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMER_CONV_MESSAGE_BIG(\n",
      "  (initial_conv): TransformerConv(50, 200, heads=4)\n",
      "  (conv1): TransformerConv(800, 200, heads=4)\n",
      "  (conv2): TransformerConv(800, 200, heads=4)\n",
      "  (conv3): TransformerConv(800, 200, heads=4)\n",
      "  (conv4): TransformerConv(800, 200, heads=4)\n",
      "  (bottleneck): Linear(in_features=200, out_features=2000, bias=True)\n",
      "  (skip1): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip2): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip3): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip4): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip5): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip6): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip7): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (relu_out_resnet): ReLU()\n",
      "  (forward_prediction): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "  (backward_prediction): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "  (gate): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "  (relu_out): ReLU()\n",
      ")\n",
      "Number of parameters:  72446600\n"
     ]
    }
   ],
   "source": [
    "class TRANSFORMER_CONV_MESSAGE_BIG(torch.nn.Module):\n",
    "    def __init__(self, heads, dropout):\n",
    "        # Init parent\n",
    "        super(TRANSFORMER_CONV_MESSAGE_BIG, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        EMBEDDING_SIZE_REDUCED = int(EMBEDDING_SIZE*0.1)\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = TransformerConv(NODE_FEATURES, EMBEDDING_SIZE_REDUCED, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        self.conv1 = TransformerConv(EMBEDDING_SIZE_REDUCED*heads, EMBEDDING_SIZE_REDUCED, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        self.conv2 = TransformerConv(EMBEDDING_SIZE_REDUCED*heads, EMBEDDING_SIZE_REDUCED, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        self.conv3 = TransformerConv(EMBEDDING_SIZE_REDUCED*heads, EMBEDDING_SIZE_REDUCED, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        self.conv4 = TransformerConv(EMBEDDING_SIZE_REDUCED*heads, EMBEDDING_SIZE_REDUCED, concat=False, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        \n",
    "        self.bottleneck = Linear(EMBEDDING_SIZE_REDUCED, EMBEDDING_SIZE)\n",
    "\n",
    "        self.skip1 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip2 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip3 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip4 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip5 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip6 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip7 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.relu_out_resnet = nn.ReLU()\n",
    "\n",
    "        self.forward_prediction = Linear(EMBEDDING_SIZE, OUTPUT_SIZE)\n",
    "        self.backward_prediction = Linear(EMBEDDING_SIZE, OUTPUT_SIZE)\n",
    "        self.gate = Linear(EMBEDDING_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "        self.relu_out = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, total_mass, batch_index):\n",
    "        \n",
    "        hidden = self.initial_conv(x, edge_index, edge_weight)\n",
    "        hidden = F.relu(hidden)\n",
    "     \n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index, edge_weight)\n",
    "        hidden = F.relu(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index, edge_weight)\n",
    "        hidden = F.relu(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index, edge_weight)\n",
    "        hidden = F.relu(hidden)\n",
    "        hidden = self.conv4(hidden, edge_index, edge_weight)\n",
    "        \n",
    "      \n",
    "        hidden = gap(hidden, batch_index)\n",
    "        hidden = self.bottleneck(hidden)\n",
    "\n",
    "        hidden = self.skip1(hidden)\n",
    "        hidden = self.skip2(hidden)\n",
    "        hidden = self.skip3(hidden)\n",
    "        hidden = self.skip4(hidden)\n",
    "        hidden = self.skip5(hidden)\n",
    "        hidden = self.skip6(hidden)\n",
    "        hidden = self.skip7(hidden)\n",
    "        \n",
    "        hidden = self.relu_out_resnet(hidden)\n",
    "\n",
    "        # Bidirectional layer\n",
    "        # Forward prediction\n",
    "        forward_prediction_hidden = self.forward_prediction(hidden)\n",
    "        forward_prediction_hidden = mask_prediction_by_mass(total_mass, forward_prediction_hidden, MASS_SHIFT)\n",
    "        \n",
    "        # # Backward prediction\n",
    "        backward_prediction_hidden = self.backward_prediction(hidden)\n",
    "        backward_prediction_hidden = reverse_prediction(total_mass, backward_prediction_hidden, MASS_SHIFT)\n",
    "        \n",
    "        # # Gate\n",
    "        gate_hidden = self.gate(hidden)\n",
    "        gate_hidden = F.sigmoid(gate_hidden)\n",
    "\n",
    "        # # Apply a final (linear) classifier.\n",
    "        out = gate_hidden * forward_prediction_hidden + (1. - gate_hidden) * backward_prediction_hidden\n",
    "        out = self.relu_out(out)\n",
    "        \n",
    "        out = out.type(torch.float64)\n",
    "        return out\n",
    "\n",
    "heads = 4\n",
    "dropout = 0.1\n",
    "model = TRANSFORMER_CONV_MESSAGE_BIG(heads, dropout)\n",
    "MODEL_NAME = \"TRANSFORMER_CONV_MESSAGE_BIG_POW\"\n",
    "MODEL_SAVE = os.path.join(BASE_DIRECTORY, MODEL_NAME)\n",
    "os.makedirs(MODEL_SAVE, mode=0o777, exist_ok=True)\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98Jii9ZP4c6n"
   },
   "outputs": [],
   "source": [
    "# alternative loss for testing\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, mass_scale=1.0,\n",
    "                 intensity_pow=0.5, pred_eps = 1e-9,\n",
    "                 **kwargs):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "\n",
    "        self.mass_scale = mass_scale\n",
    "        self.intensity_pow = intensity_pow\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.pred_eps = pred_eps\n",
    "\n",
    "    def forward(self, true_spect, pred_spect, **kwargs):\n",
    "        SPECT_N = true_spect.shape[1]\n",
    "\n",
    "        w = torch.arange(SPECT_N).to(true_spect.device) * self.mass_scale        \n",
    "\n",
    "        eps = self.pred_eps\n",
    "        \n",
    "        pred_weighted = (pred_spect+eps)**self.intensity_pow\n",
    "        pred_weighted_norm = torch.sqrt((pred_weighted**2).sum(dim=1)).unsqueeze(-1)\n",
    "        \n",
    "        true_weighted = (true_spect+eps)**self.intensity_pow\n",
    "        true_weighted_norm = torch.sqrt((true_weighted**2).sum(dim=1)).unsqueeze(-1)\n",
    "\n",
    "        pred_weighted_normed = pred_weighted / pred_weighted_norm\n",
    "        true_weighted_normed = true_weighted / true_weighted_norm\n",
    "        \n",
    "\n",
    "        return self.loss(pred_weighted_normed, true_weighted_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7E1ftolQo4ri"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjbzcPf1lQfZ",
    "outputId": "1da7c2f7-c91e-467d-ac94-b0abc95debe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 | Test DotSimilarity is 0.763147555865506\n",
      "Epoch 0 | Validation DotSimilarity is 0.7628311546732448\n",
      "Epoch 0 | Train Loss 0.0004727543195389215\n",
      "\n",
      "Epoch 1 | Test DotSimilarity is 0.7892757248235717\n",
      "Epoch 1 | Validation DotSimilarity is 0.7895821529579218\n",
      "Epoch 1 | Train Loss 0.000392899382346316\n",
      "\n",
      "Epoch 2 | Test DotSimilarity is 0.8057254113792304\n",
      "Epoch 2 | Validation DotSimilarity is 0.806004839242652\n",
      "Epoch 2 | Train Loss 0.0003604965866350077\n",
      "\n",
      "Epoch 3 | Test DotSimilarity is 0.8127264122361272\n",
      "Epoch 3 | Validation DotSimilarity is 0.8131049756843344\n",
      "Epoch 3 | Train Loss 0.0003399959321121596\n",
      "\n",
      "Epoch 4 | Test DotSimilarity is 0.8205465654722626\n",
      "Epoch 4 | Validation DotSimilarity is 0.8204107633493962\n",
      "Epoch 4 | Train Loss 0.0003275351155965237\n",
      "\n",
      "Epoch 5 | Test DotSimilarity is 0.8256365048397596\n",
      "Epoch 5 | Validation DotSimilarity is 0.8258806002680329\n",
      "Epoch 5 | Train Loss 0.00031328520054535024\n",
      "\n",
      "Epoch 6 | Test DotSimilarity is 0.8319696746080616\n",
      "Epoch 6 | Validation DotSimilarity is 0.832448628024774\n",
      "Epoch 6 | Train Loss 0.0003022935339398318\n",
      "\n",
      "Epoch 7 | Test DotSimilarity is 0.8356306946659888\n",
      "Epoch 7 | Validation DotSimilarity is 0.8358510027846631\n",
      "Epoch 7 | Train Loss 0.00029300368825129836\n",
      "\n",
      "Epoch 8 | Test DotSimilarity is 0.838724089321871\n",
      "Epoch 8 | Validation DotSimilarity is 0.8389026701591887\n",
      "Epoch 8 | Train Loss 0.0002862745837383503\n",
      "\n",
      "Epoch 9 | Test DotSimilarity is 0.8416009017878606\n",
      "Epoch 9 | Validation DotSimilarity is 0.8415861020409673\n",
      "Epoch 9 | Train Loss 0.00027845439366764636\n",
      "\n",
      "Epoch 10 | Test DotSimilarity is 0.8427132769038981\n",
      "Epoch 10 | Validation DotSimilarity is 0.8429645386384927\n",
      "Epoch 10 | Train Loss 0.00027180610933714614\n",
      "\n",
      "Epoch 11 | Test DotSimilarity is 0.8455546411208062\n",
      "Epoch 11 | Validation DotSimilarity is 0.845308997384266\n",
      "Epoch 11 | Train Loss 0.0002661644451835218\n",
      "\n",
      "Epoch 12 | Test DotSimilarity is 0.8467356528431672\n",
      "Epoch 12 | Validation DotSimilarity is 0.8468671042839716\n",
      "Epoch 12 | Train Loss 0.00026002347011887554\n",
      "\n",
      "Epoch 13 | Test DotSimilarity is 0.847783263721532\n",
      "Epoch 13 | Validation DotSimilarity is 0.8482078200496286\n",
      "Epoch 13 | Train Loss 0.0002551773987424725\n",
      "\n",
      "Epoch 14 | Test DotSimilarity is 0.8494478879901668\n",
      "Epoch 14 | Validation DotSimilarity is 0.8504471730666584\n",
      "Epoch 14 | Train Loss 0.0002496167017119147\n",
      "\n",
      "Epoch 15 | Test DotSimilarity is 0.8455959066076526\n",
      "Epoch 15 | Validation DotSimilarity is 0.8467480295961078\n",
      "Epoch 15 | Train Loss 0.00024542824440699693\n",
      "\n",
      "Epoch 16 | Test DotSimilarity is 0.8515280513147098\n",
      "Epoch 16 | Validation DotSimilarity is 0.8517258549820461\n",
      "Epoch 16 | Train Loss 0.00024248922452972286\n",
      "\n",
      "Epoch 17 | Test DotSimilarity is 0.850638060113287\n",
      "Epoch 17 | Validation DotSimilarity is 0.8514440819914587\n",
      "Epoch 17 | Train Loss 0.0002384960084066415\n",
      "\n",
      "Epoch 18 | Test DotSimilarity is 0.8513639649807218\n",
      "Epoch 18 | Validation DotSimilarity is 0.8525991969699739\n",
      "Epoch 18 | Train Loss 0.00023305607666680375\n",
      "\n",
      "Epoch 19 | Test DotSimilarity is 0.8502145526948891\n",
      "Epoch 19 | Validation DotSimilarity is 0.8508907649151235\n",
      "Epoch 19 | Train Loss 0.0002302100347853848\n",
      "\n",
      "Epoch 20 | Test DotSimilarity is 0.8524260945437981\n",
      "Epoch 20 | Validation DotSimilarity is 0.8531139376419694\n",
      "Epoch 20 | Train Loss 0.00022765176855532178\n",
      "\n",
      "Epoch 21 | Test DotSimilarity is 0.8514656291343088\n",
      "Epoch 21 | Validation DotSimilarity is 0.8520140548625103\n",
      "Epoch 21 | Train Loss 0.0002238690862865182\n",
      "\n",
      "Epoch 22 | Test DotSimilarity is 0.852139930153784\n",
      "Epoch 22 | Validation DotSimilarity is 0.8540323803089854\n",
      "Epoch 22 | Train Loss 0.00022112858694831565\n",
      "\n",
      "Epoch 23 | Test DotSimilarity is 0.8531095680807564\n",
      "Epoch 23 | Validation DotSimilarity is 0.8540040755076436\n",
      "Epoch 23 | Train Loss 0.00021679612658881135\n",
      "\n",
      "Epoch 24 | Test DotSimilarity is 0.8546402004066495\n",
      "Epoch 24 | Validation DotSimilarity is 0.855872555603236\n",
      "Epoch 24 | Train Loss 0.00021357120609228348\n",
      "\n",
      "Epoch 25 | Test DotSimilarity is 0.8537974789757327\n",
      "Epoch 25 | Validation DotSimilarity is 0.8549002259295532\n",
      "Epoch 25 | Train Loss 0.0002102045483319389\n",
      "\n",
      "Epoch 26 | Test DotSimilarity is 0.8537821540908274\n",
      "Epoch 26 | Validation DotSimilarity is 0.8550910399889287\n",
      "Epoch 26 | Train Loss 0.0002072217789419892\n",
      "\n",
      "Epoch 27 | Test DotSimilarity is 0.8537914517464966\n",
      "Epoch 27 | Validation DotSimilarity is 0.8555816306793785\n",
      "Epoch 27 | Train Loss 0.00020561651998690696\n",
      "\n",
      "Epoch 28 | Test DotSimilarity is 0.8542694169899332\n",
      "Epoch 28 | Validation DotSimilarity is 0.8554259719650256\n",
      "Epoch 28 | Train Loss 0.00020268608611375384\n",
      "\n",
      "Epoch 29 | Test DotSimilarity is 0.8549755300794499\n",
      "Epoch 29 | Validation DotSimilarity is 0.8554333307939909\n",
      "Epoch 29 | Train Loss 0.00019949886910923168\n",
      "\n",
      "Epoch 30 | Test DotSimilarity is 0.8536348332330715\n",
      "Epoch 30 | Validation DotSimilarity is 0.8547751295016145\n",
      "Epoch 30 | Train Loss 0.00019697402520368033\n",
      "\n",
      "Epoch 31 | Test DotSimilarity is 0.8544030645850808\n",
      "Epoch 31 | Validation DotSimilarity is 0.8555166514640048\n",
      "Epoch 31 | Train Loss 0.00019570669137925568\n",
      "\n",
      "Epoch 32 | Test DotSimilarity is 0.8550784175011482\n",
      "Epoch 32 | Validation DotSimilarity is 0.855879449447737\n",
      "Epoch 32 | Train Loss 0.00019209959983419237\n",
      "\n",
      "Epoch 33 | Test DotSimilarity is 0.8542137376187143\n",
      "Epoch 33 | Validation DotSimilarity is 0.8556720892939007\n",
      "Epoch 33 | Train Loss 0.00019137635546666475\n",
      "\n",
      "Epoch 34 | Test DotSimilarity is 0.8538840750292719\n",
      "Epoch 34 | Validation DotSimilarity is 0.8549527011686292\n",
      "Epoch 34 | Train Loss 0.0001887440043094349\n",
      "\n",
      "Epoch 35 | Test DotSimilarity is 0.8550511210593484\n",
      "Epoch 35 | Validation DotSimilarity is 0.8563532728302514\n",
      "Epoch 35 | Train Loss 0.00018567131340554923\n",
      "\n",
      "Epoch 36 | Test DotSimilarity is 0.8544849309911622\n",
      "Epoch 36 | Validation DotSimilarity is 0.8556523574605575\n",
      "Epoch 36 | Train Loss 0.00018531716112901587\n",
      "\n",
      "Epoch 37 | Test DotSimilarity is 0.853466156166493\n",
      "Epoch 37 | Validation DotSimilarity is 0.8549776386047111\n",
      "Epoch 37 | Train Loss 0.0001823650817482045\n",
      "\n",
      "Epoch 38 | Test DotSimilarity is 0.8544462856768452\n",
      "Epoch 38 | Validation DotSimilarity is 0.8559167722090336\n",
      "Epoch 38 | Train Loss 0.00018057519848198845\n",
      "\n",
      "Epoch 39 | Test DotSimilarity is 0.8550269332213284\n",
      "Epoch 39 | Validation DotSimilarity is 0.8562667899429376\n",
      "Epoch 39 | Train Loss 0.0001793464712637147\n",
      "\n",
      "Epoch 40 | Test DotSimilarity is 0.8550811435241871\n",
      "Epoch 40 | Validation DotSimilarity is 0.8558856892946555\n",
      "Epoch 40 | Train Loss 0.00017703624071755718\n",
      "\n",
      "Epoch 41 | Test DotSimilarity is 0.8546220035877735\n",
      "Epoch 41 | Validation DotSimilarity is 0.8558215187232058\n",
      "Epoch 41 | Train Loss 0.00017468920285601202\n",
      "\n",
      "Epoch 42 | Test DotSimilarity is 0.8543621905286723\n",
      "Epoch 42 | Validation DotSimilarity is 0.8555697801706105\n",
      "Epoch 42 | Train Loss 0.00017215173310848443\n",
      "\n",
      "Epoch 43 | Test DotSimilarity is 0.8542366790956654\n",
      "Epoch 43 | Validation DotSimilarity is 0.8552618354125238\n",
      "Epoch 43 | Train Loss 0.00017192645999061676\n",
      "\n",
      "Epoch 44 | Test DotSimilarity is 0.8546589140763498\n",
      "Epoch 44 | Validation DotSimilarity is 0.8565848884855659\n",
      "Epoch 44 | Train Loss 0.0001695184719796899\n",
      "\n",
      "Epoch 45 | Test DotSimilarity is 0.8555705607431507\n",
      "Epoch 45 | Validation DotSimilarity is 0.8565079578429381\n",
      "Epoch 45 | Train Loss 0.00016828187680868615\n",
      "\n",
      "Epoch 46 | Test DotSimilarity is 0.8542989234314741\n",
      "Epoch 46 | Validation DotSimilarity is 0.8554895271545349\n",
      "Epoch 46 | Train Loss 0.00016781203073561776\n",
      "\n",
      "Epoch 47 | Test DotSimilarity is 0.8545940409429708\n",
      "Epoch 47 | Validation DotSimilarity is 0.8556385555660243\n",
      "Epoch 47 | Train Loss 0.00016540188890568346\n",
      "\n",
      "Epoch 48 | Test DotSimilarity is 0.8544597092286491\n",
      "Epoch 48 | Validation DotSimilarity is 0.8563415316730923\n",
      "Epoch 48 | Train Loss 0.00016348476338833996\n",
      "\n",
      "Epoch 49 | Test DotSimilarity is 0.8544771695458736\n",
      "Epoch 49 | Validation DotSimilarity is 0.8560341874042396\n",
      "Epoch 49 | Train Loss 0.00016248485608409412\n",
      "\n",
      "Epoch 50 | Test DotSimilarity is 0.8555100118856938\n",
      "Epoch 50 | Validation DotSimilarity is 0.8568724605327154\n",
      "Epoch 50 | Train Loss 0.00016040654550240077\n",
      "\n",
      "Epoch 51 | Test DotSimilarity is 0.8547579737217172\n",
      "Epoch 51 | Validation DotSimilarity is 0.8567522839044861\n",
      "Epoch 51 | Train Loss 0.00015928006904956672\n",
      "\n",
      "Epoch 52 | Test DotSimilarity is 0.8548524857187821\n",
      "Epoch 52 | Validation DotSimilarity is 0.8565500859942459\n",
      "Epoch 52 | Train Loss 0.00015775004697001738\n",
      "\n",
      "Epoch 53 | Test DotSimilarity is 0.8554454836291169\n",
      "Epoch 53 | Validation DotSimilarity is 0.8568650483511429\n",
      "Epoch 53 | Train Loss 0.0001566344389541924\n",
      "\n",
      "Epoch 54 | Test DotSimilarity is 0.8546939146161916\n",
      "Epoch 54 | Validation DotSimilarity is 0.856057860708648\n",
      "Epoch 54 | Train Loss 0.00015528296326608137\n",
      "\n",
      "Epoch 55 | Test DotSimilarity is 0.8539517260003344\n",
      "Epoch 55 | Validation DotSimilarity is 0.8564680437138253\n",
      "Epoch 55 | Train Loss 0.00015460008174510964\n",
      "\n",
      "Epoch 56 | Test DotSimilarity is 0.8552369844202509\n",
      "Epoch 56 | Validation DotSimilarity is 0.8565767273693655\n",
      "Epoch 56 | Train Loss 0.00015280918324836904\n",
      "\n",
      "Epoch 57 | Test DotSimilarity is 0.8550184256903262\n",
      "Epoch 57 | Validation DotSimilarity is 0.8569151838557718\n",
      "Epoch 57 | Train Loss 0.00015118392334481951\n",
      "\n",
      "Epoch 58 | Test DotSimilarity is 0.8543956988558824\n",
      "Epoch 58 | Validation DotSimilarity is 0.8560683214135865\n",
      "Epoch 58 | Train Loss 0.00014994111160046187\n",
      "\n",
      "Epoch 59 | Test DotSimilarity is 0.8541092219428625\n",
      "Epoch 59 | Validation DotSimilarity is 0.8551469331366564\n",
      "Epoch 59 | Train Loss 0.0001496581232415293\n",
      "\n",
      "Epoch 60 | Test DotSimilarity is 0.85443905671969\n",
      "Epoch 60 | Validation DotSimilarity is 0.8565426269270753\n",
      "Epoch 60 | Train Loss 0.00014828600670562195\n",
      "\n",
      "Epoch 61 | Test DotSimilarity is 0.8547029735254109\n",
      "Epoch 61 | Validation DotSimilarity is 0.8566027989987378\n",
      "Epoch 61 | Train Loss 0.0001473595362044483\n",
      "\n",
      "Epoch 62 | Test DotSimilarity is 0.8548558213935455\n",
      "Epoch 62 | Validation DotSimilarity is 0.8568085716214697\n",
      "Epoch 62 | Train Loss 0.000146115476597722\n",
      "\n",
      "Epoch 63 | Test DotSimilarity is 0.8544424717678616\n",
      "Epoch 63 | Validation DotSimilarity is 0.8560752118398101\n",
      "Epoch 63 | Train Loss 0.0001446277175821969\n",
      "\n",
      "Epoch 64 | Test DotSimilarity is 0.8550977605788059\n",
      "Epoch 64 | Validation DotSimilarity is 0.8565644649583668\n",
      "Epoch 64 | Train Loss 0.00014326850208201347\n",
      "\n",
      "Epoch 65 | Test DotSimilarity is 0.8553747550563472\n",
      "Epoch 65 | Validation DotSimilarity is 0.856770017441464\n",
      "Epoch 65 | Train Loss 0.0001432093959356042\n",
      "\n",
      "Epoch 66 | Test DotSimilarity is 0.854256068257225\n",
      "Epoch 66 | Validation DotSimilarity is 0.855428253648225\n",
      "Epoch 66 | Train Loss 0.00014174592433545546\n",
      "\n",
      "Epoch 67 | Test DotSimilarity is 0.8553685825035149\n",
      "Epoch 67 | Validation DotSimilarity is 0.8563943257679344\n",
      "Epoch 67 | Train Loss 0.0001408640036814007\n",
      "\n",
      "Epoch 68 | Test DotSimilarity is 0.8532599115068722\n",
      "Epoch 68 | Validation DotSimilarity is 0.8543311319631601\n",
      "Epoch 68 | Train Loss 0.00013961998382772744\n",
      "\n",
      "Epoch 69 | Test DotSimilarity is 0.8553119908001788\n",
      "Epoch 69 | Validation DotSimilarity is 0.8565805637212861\n",
      "Epoch 69 | Train Loss 0.00013923976116103173\n",
      "\n",
      "Epoch 70 | Test DotSimilarity is 0.8552304120590097\n",
      "Epoch 70 | Validation DotSimilarity is 0.8563119864690397\n",
      "Epoch 70 | Train Loss 0.0001379277635880836\n",
      "\n",
      "Epoch 71 | Test DotSimilarity is 0.8551733978950712\n",
      "Epoch 71 | Validation DotSimilarity is 0.8557009114681531\n",
      "Epoch 71 | Train Loss 0.00013706804461988996\n",
      "\n",
      "Epoch 72 | Test DotSimilarity is 0.8564890854078919\n",
      "Epoch 72 | Validation DotSimilarity is 0.8574906592039079\n",
      "Epoch 72 | Train Loss 0.00013668658605421954\n",
      "\n",
      "Epoch 73 | Test DotSimilarity is 0.854803192113305\n",
      "Epoch 73 | Validation DotSimilarity is 0.8562381176005416\n",
      "Epoch 73 | Train Loss 0.00013632474618780217\n",
      "\n",
      "Epoch 74 | Test DotSimilarity is 0.8562547507352294\n",
      "Epoch 74 | Validation DotSimilarity is 0.8576523260100805\n",
      "Epoch 74 | Train Loss 0.00013597368167536734\n",
      "\n",
      "Epoch 75 | Test DotSimilarity is 0.8544239045777727\n",
      "Epoch 75 | Validation DotSimilarity is 0.8560397537756231\n",
      "Epoch 75 | Train Loss 0.00013404986788071637\n",
      "\n",
      "Epoch 76 | Test DotSimilarity is 0.8546295557940983\n",
      "Epoch 76 | Validation DotSimilarity is 0.8557647850286884\n",
      "Epoch 76 | Train Loss 0.0001345033999968765\n",
      "\n",
      "Epoch 77 | Test DotSimilarity is 0.8557871066459567\n",
      "Epoch 77 | Validation DotSimilarity is 0.8573184742588686\n",
      "Epoch 77 | Train Loss 0.0001329375482179503\n",
      "\n",
      "Epoch 78 | Test DotSimilarity is 0.8551994835310459\n",
      "Epoch 78 | Validation DotSimilarity is 0.8561010995601378\n",
      "Epoch 78 | Train Loss 0.0001323652877071069\n",
      "\n",
      "Epoch 79 | Test DotSimilarity is 0.8540466411716737\n",
      "Epoch 79 | Validation DotSimilarity is 0.8550659665762543\n",
      "Epoch 79 | Train Loss 0.00013173448891150592\n",
      "\n",
      "Epoch 80 | Test DotSimilarity is 0.8546188282968545\n",
      "Epoch 80 | Validation DotSimilarity is 0.8560549647334997\n",
      "Epoch 80 | Train Loss 0.0001308652594922613\n",
      "\n",
      "Epoch 81 | Test DotSimilarity is 0.8555093165922856\n",
      "Epoch 81 | Validation DotSimilarity is 0.8557607407030733\n",
      "Epoch 81 | Train Loss 0.00013031009340080772\n",
      "\n",
      "Epoch 82 | Test DotSimilarity is 0.8559176052089695\n",
      "Epoch 82 | Validation DotSimilarity is 0.8569128923697473\n",
      "Epoch 82 | Train Loss 0.0001291777325344393\n",
      "\n",
      "Epoch 83 | Test DotSimilarity is 0.8547014147032435\n",
      "Epoch 83 | Validation DotSimilarity is 0.8557972602946909\n",
      "Epoch 83 | Train Loss 0.00013086416322792167\n",
      "\n",
      "Epoch 84 | Test DotSimilarity is 0.8554090385163734\n",
      "Epoch 84 | Validation DotSimilarity is 0.856144729584522\n",
      "Epoch 84 | Train Loss 0.00012805549820001105\n",
      "\n",
      "Epoch 85 | Test DotSimilarity is 0.8547470622849156\n",
      "Epoch 85 | Validation DotSimilarity is 0.8548242972493972\n",
      "Epoch 85 | Train Loss 0.00012727874073803952\n",
      "\n",
      "Epoch 86 | Test DotSimilarity is 0.8557567111162513\n",
      "Epoch 86 | Validation DotSimilarity is 0.856946122159978\n",
      "Epoch 86 | Train Loss 0.00012738375786624078\n",
      "\n",
      "Epoch 87 | Test DotSimilarity is 0.8548667323150803\n",
      "Epoch 87 | Validation DotSimilarity is 0.8557191723648151\n",
      "Epoch 87 | Train Loss 0.00012621730290575365\n",
      "\n",
      "Epoch 88 | Test DotSimilarity is 0.8556083337308753\n",
      "Epoch 88 | Validation DotSimilarity is 0.8573417890862842\n",
      "Epoch 88 | Train Loss 0.00012541449282779012\n",
      "\n",
      "Epoch 89 | Test DotSimilarity is 0.8533282972090759\n",
      "Epoch 89 | Validation DotSimilarity is 0.8546365393008866\n",
      "Epoch 89 | Train Loss 0.00012491887840547982\n",
      "\n",
      "Epoch 90 | Test DotSimilarity is 0.8548571891304888\n",
      "Epoch 90 | Validation DotSimilarity is 0.8566504564184175\n",
      "Epoch 90 | Train Loss 0.00012459310968100776\n",
      "\n",
      "Epoch 91 | Test DotSimilarity is 0.8554058826635579\n",
      "Epoch 91 | Validation DotSimilarity is 0.8568137711632356\n",
      "Epoch 91 | Train Loss 0.00012335460835322237\n",
      "\n",
      "Epoch 92 | Test DotSimilarity is 0.8556413071723431\n",
      "Epoch 92 | Validation DotSimilarity is 0.8565969198252015\n",
      "Epoch 92 | Train Loss 0.00012300339297958605\n",
      "\n",
      "Epoch 93 | Test DotSimilarity is 0.8555618075951985\n",
      "Epoch 93 | Validation DotSimilarity is 0.8573060955247492\n",
      "Epoch 93 | Train Loss 0.00012271906371269745\n",
      "\n",
      "Epoch 94 | Test DotSimilarity is 0.8558936151109822\n",
      "Epoch 94 | Validation DotSimilarity is 0.8577311133261177\n",
      "Epoch 94 | Train Loss 0.00012141525324154968\n",
      "\n",
      "Epoch 95 | Test DotSimilarity is 0.8556859401370608\n",
      "Epoch 95 | Validation DotSimilarity is 0.8573160013866146\n",
      "Epoch 95 | Train Loss 0.00012134432183788984\n",
      "\n",
      "Epoch 96 | Test DotSimilarity is 0.8556637501064023\n",
      "Epoch 96 | Validation DotSimilarity is 0.856300537129767\n",
      "Epoch 96 | Train Loss 0.00012044892982294569\n",
      "\n",
      "Epoch 97 | Test DotSimilarity is 0.8553672917536532\n",
      "Epoch 97 | Validation DotSimilarity is 0.857347105919645\n",
      "Epoch 97 | Train Loss 0.00012031923525528423\n",
      "\n",
      "Epoch 98 | Test DotSimilarity is 0.8544607742575606\n",
      "Epoch 98 | Validation DotSimilarity is 0.8556743089777319\n",
      "Epoch 98 | Train Loss 0.00012188995979015351\n",
      "\n",
      "Epoch 99 | Test DotSimilarity is 0.8570886741222795\n",
      "Epoch 99 | Validation DotSimilarity is 0.8584194547346436\n",
      "Epoch 99 | Train Loss 0.00011316257465760126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "######################################\n",
    "#  LOSS\n",
    "######################################\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.HuberLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005) \n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "\n",
    "# Use GPU for training\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap data in a data loader\n",
    "\n",
    "\n",
    "\n",
    "NUM_GRAPHS_PER_BATCH = 64\n",
    "\n",
    "\n",
    "loader = DataLoader(train_dataset, \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "SAVE_EVERY_X_EPOCH = 10\n",
    "REPORT_EVERY_X_EPOCH = 1\n",
    "\n",
    "def train(loader):\n",
    "    # Enumerate over the data\n",
    "    loss_per_batch = np.array([])\n",
    "    for batch in loader:\n",
    "        # Use GPU\n",
    "        batch.to(device)  \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Passing the node features and the connection info\n",
    "        pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch) \n",
    "        # Calculating the loss and gradients\n",
    "        loss = loss_fn(pred, batch.y)\n",
    "        loss.backward()  \n",
    "        # Update using the gradients\n",
    "        optimizer.step()\n",
    "        loss_per_batch = np.concatenate((loss_per_batch, np.array([loss.clone().detach().cpu().numpy()])))\n",
    "    return loss_per_batch\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(300):\n",
    "    scheduler.step()\n",
    "    loss = train(loader)\n",
    "    pred_test_similarity = np.array([])\n",
    "    pred_validation_similarity = np.array([])\n",
    "   \n",
    "   \n",
    "    if epoch % REPORT_EVERY_X_EPOCH == 0:\n",
    "        for batch in test_loader:\n",
    "            batch.to(device)  \n",
    "            pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch)\n",
    "\n",
    "            batch_similarity = validate_similarities(batch.y.detach().cpu().numpy(),\n",
    "                                  pred.detach().cpu().numpy(),\n",
    "                                  mass_pow=1.0, intensity_pow=0.5)\n",
    "\n",
    "            pred_test_similarity = np.concatenate((pred_test_similarity, batch_similarity))\n",
    "    \n",
    "        for batch in validation_loader:\n",
    "           \n",
    "            batch.to(device)  \n",
    "            pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch)\n",
    "\n",
    "            batch_similarity = validate_similarities(batch.y.detach().cpu().numpy(),\n",
    "                                  pred.detach().cpu().numpy(),\n",
    "                                  mass_pow=1.0, intensity_pow=0.5)\n",
    "\n",
    "            pred_validation_similarity = np.concatenate((pred_validation_similarity, batch_similarity))\n",
    "            \n",
    "        \n",
    "        print(f\"Epoch {epoch} | Test DotSimilarity is {pred_test_similarity.mean()}\")\n",
    "        print(f\"Epoch {epoch} | Validation DotSimilarity is {pred_validation_similarity.mean()}\")\n",
    "        print(f\"Epoch {epoch} | Train Loss {loss.mean()}\")\n",
    "        print()\n",
    "\n",
    "    \n",
    "    if epoch % SAVE_EVERY_X_EPOCH == 0:\n",
    "        SAVE_PATH = f\"{epoch}.pt\"\n",
    "            \n",
    "        # Save model\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'metadata': {\"loss\" : \"HuberLoss\",\n",
    "                     \"Dataset\": \"Preprocessed_test_pow_preparation_no_sparse_small\",\n",
    "                     \"test_similarities\": pred_test_similarity.mean()}\n",
    "        }, os.path.join(MODEL_SAVE, SAVE_PATH))\n",
    "\n",
    "        LOSS_FILE = f\"all_loss_until_{epoch}.output\"\n",
    "        with open(os.path.join(MODEL_SAVE, LOSS_FILE), 'wb') as fid:\n",
    "            pickle.dump(loss.mean(), fid)\n",
    "            fid.close() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cQZ0-1tlQhu"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMh-oyM9lQjr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2M31E-jzXk-"
   },
   "source": [
    "# CONTINUE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLH82e3LlQnW"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/NIST_SMALL/train_subset_pow.pkl\", 'rb') as handle:\n",
    "   train_dataset  = pickle.load(handle)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/NIST_SMALL/validation_subset_pow.pkl\", 'rb') as handle:\n",
    "   validation_dataset  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9M1OfPEk0VYE"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/NIST_SMALL/Preprocessed_test_pow_preparation_no_sparse_small.output\", 'rb') as handle:\n",
    "   data_list_test  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEZlEMuH0Pxh"
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_VALIDATION = 5000\n",
    "test_dataset = data_list_test[:NUMBER_OF_VALIDATION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdMgClDHlQpG",
    "outputId": "2d581621-8a9e-4e65-8b6b-29473129a6d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRANSFORMER_CONV_MESSAGE_BIG(\n",
       "  (initial_conv): TransformerConv(50, 200, heads=4)\n",
       "  (conv1): TransformerConv(800, 200, heads=4)\n",
       "  (conv2): TransformerConv(800, 200, heads=4)\n",
       "  (conv3): TransformerConv(800, 200, heads=4)\n",
       "  (conv4): TransformerConv(800, 200, heads=4)\n",
       "  (bottleneck): Linear(in_features=200, out_features=2000, bias=True)\n",
       "  (skip1): SKIPblock(\n",
       "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (relu1): ReLU()\n",
       "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (skip2): SKIPblock(\n",
       "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (relu1): ReLU()\n",
       "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (skip3): SKIPblock(\n",
       "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (relu1): ReLU()\n",
       "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (skip4): SKIPblock(\n",
       "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (relu1): ReLU()\n",
       "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (skip5): SKIPblock(\n",
       "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (relu1): ReLU()\n",
       "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (skip6): SKIPblock(\n",
       "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (relu1): ReLU()\n",
       "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (skip7): SKIPblock(\n",
       "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (relu1): ReLU()\n",
       "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (relu_out_resnet): ReLU()\n",
       "  (forward_prediction): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "  (backward_prediction): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "  (gate): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "  (relu_out): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = 4\n",
    "dropout = 0.1\n",
    "model = TRANSFORMER_CONV_MESSAGE_BIG(heads, dropout)\n",
    "MODEL_NAME = \"TRANSFORMER_CONV_MESSAGE_BIG_POW\"\n",
    "\n",
    "checkpoint = torch.load(\"/content/drive/MyDrive/NIST_SMALL/TRANSFORMER_CONV_MESSAGE_BIG_POW/190.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIhaYr6DlQq3",
    "outputId": "fda31e06-eaaf-4ee9-ae2d-1841cda9da3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 190 | Test DotSimilarity is 0.8401882334899837\n",
      "Epoch 190 | Validation DotSimilarity is 0.8391219171194096\n",
      "Epoch 190 | Train Loss 0.027190552639433558\n",
      "\n",
      "Epoch 191 | Test DotSimilarity is 0.8398562720186644\n",
      "Epoch 191 | Validation DotSimilarity is 0.8390803237848616\n",
      "Epoch 191 | Train Loss 0.027785486662701652\n",
      "\n",
      "Epoch 192 | Test DotSimilarity is 0.8403587006449177\n",
      "Epoch 192 | Validation DotSimilarity is 0.8390011866536925\n",
      "Epoch 192 | Train Loss 0.027472053621506592\n",
      "\n",
      "Epoch 193 | Test DotSimilarity is 0.8400377613055745\n",
      "Epoch 193 | Validation DotSimilarity is 0.8393120387281294\n",
      "Epoch 193 | Train Loss 0.027493677934210644\n",
      "\n",
      "Epoch 194 | Test DotSimilarity is 0.8409149199666491\n",
      "Epoch 194 | Validation DotSimilarity is 0.8401056861256162\n",
      "Epoch 194 | Train Loss 0.027430074677446105\n",
      "\n",
      "Epoch 195 | Test DotSimilarity is 0.8412649650209685\n",
      "Epoch 195 | Validation DotSimilarity is 0.8401463431309641\n",
      "Epoch 195 | Train Loss 0.02720610222731332\n",
      "\n",
      "Epoch 196 | Test DotSimilarity is 0.8404639380558044\n",
      "Epoch 196 | Validation DotSimilarity is 0.8395093633968863\n",
      "Epoch 196 | Train Loss 0.02712801467391078\n",
      "\n",
      "Epoch 197 | Test DotSimilarity is 0.8410498900292322\n",
      "Epoch 197 | Validation DotSimilarity is 0.8402038748698079\n",
      "Epoch 197 | Train Loss 0.027005023554844448\n",
      "\n",
      "Epoch 198 | Test DotSimilarity is 0.8402884741929445\n",
      "Epoch 198 | Validation DotSimilarity is 0.8401708636457436\n",
      "Epoch 198 | Train Loss 0.02679699400369585\n",
      "\n",
      "Epoch 199 | Test DotSimilarity is 0.8413703456440264\n",
      "Epoch 199 | Validation DotSimilarity is 0.8410153116183093\n",
      "Epoch 199 | Train Loss 0.02681780280831817\n",
      "\n",
      "Epoch 200 | Test DotSimilarity is 0.8410451269632063\n",
      "Epoch 200 | Validation DotSimilarity is 0.8406798575844158\n",
      "Epoch 200 | Train Loss 0.026542321440211236\n",
      "\n",
      "Epoch 201 | Test DotSimilarity is 0.8412484076527453\n",
      "Epoch 201 | Validation DotSimilarity is 0.8402900314360296\n",
      "Epoch 201 | Train Loss 0.026445494588878075\n",
      "\n",
      "Epoch 202 | Test DotSimilarity is 0.8414866186091114\n",
      "Epoch 202 | Validation DotSimilarity is 0.8411621934102878\n",
      "Epoch 202 | Train Loss 0.026243759379854157\n",
      "\n",
      "Epoch 203 | Test DotSimilarity is 0.8410362188165799\n",
      "Epoch 203 | Validation DotSimilarity is 0.8406939454482512\n",
      "Epoch 203 | Train Loss 0.026188958288322732\n",
      "\n",
      "Epoch 204 | Test DotSimilarity is 0.8412219424866442\n",
      "Epoch 204 | Validation DotSimilarity is 0.8400610287008594\n",
      "Epoch 204 | Train Loss 0.02617759560395823\n",
      "\n",
      "Epoch 205 | Test DotSimilarity is 0.841614591597248\n",
      "Epoch 205 | Validation DotSimilarity is 0.840775855067576\n",
      "Epoch 205 | Train Loss 0.02584594819416694\n",
      "\n",
      "Epoch 206 | Test DotSimilarity is 0.8417036706112881\n",
      "Epoch 206 | Validation DotSimilarity is 0.8408415353391083\n",
      "Epoch 206 | Train Loss 0.02576311523186447\n",
      "\n",
      "Epoch 207 | Test DotSimilarity is 0.8414665391798939\n",
      "Epoch 207 | Validation DotSimilarity is 0.8409442915027221\n",
      "Epoch 207 | Train Loss 0.02562935862913203\n",
      "\n",
      "Epoch 208 | Test DotSimilarity is 0.8413519604232257\n",
      "Epoch 208 | Validation DotSimilarity is 0.8405091187424683\n",
      "Epoch 208 | Train Loss 0.025467968812002455\n",
      "\n",
      "Epoch 209 | Test DotSimilarity is 0.8416610966653801\n",
      "Epoch 209 | Validation DotSimilarity is 0.8409272698311528\n",
      "Epoch 209 | Train Loss 0.025420935936950183\n",
      "\n",
      "Epoch 210 | Test DotSimilarity is 0.841543274865182\n",
      "Epoch 210 | Validation DotSimilarity is 0.8409251490652978\n",
      "Epoch 210 | Train Loss 0.025228206743581106\n",
      "\n",
      "Epoch 211 | Test DotSimilarity is 0.8415753618347269\n",
      "Epoch 211 | Validation DotSimilarity is 0.8409759971125658\n",
      "Epoch 211 | Train Loss 0.025110787949478915\n",
      "\n",
      "Epoch 212 | Test DotSimilarity is 0.8424566105209881\n",
      "Epoch 212 | Validation DotSimilarity is 0.8407791549988105\n",
      "Epoch 212 | Train Loss 0.02505381760170113\n",
      "\n",
      "Epoch 213 | Test DotSimilarity is 0.8418573097579685\n",
      "Epoch 213 | Validation DotSimilarity is 0.8406272482363522\n",
      "Epoch 213 | Train Loss 0.02489476517534261\n",
      "\n",
      "Epoch 214 | Test DotSimilarity is 0.8417122283227653\n",
      "Epoch 214 | Validation DotSimilarity is 0.8413004508936869\n",
      "Epoch 214 | Train Loss 0.024704024403191492\n",
      "\n",
      "Epoch 215 | Test DotSimilarity is 0.8415604018799365\n",
      "Epoch 215 | Validation DotSimilarity is 0.8405813447348691\n",
      "Epoch 215 | Train Loss 0.02464475827199354\n",
      "\n",
      "Epoch 216 | Test DotSimilarity is 0.8419715537999815\n",
      "Epoch 216 | Validation DotSimilarity is 0.8415059473729037\n",
      "Epoch 216 | Train Loss 0.024569415796956637\n",
      "\n",
      "Epoch 217 | Test DotSimilarity is 0.8420661709400978\n",
      "Epoch 217 | Validation DotSimilarity is 0.8409845603647744\n",
      "Epoch 217 | Train Loss 0.024419026195830505\n",
      "\n",
      "Epoch 218 | Test DotSimilarity is 0.8415282056045533\n",
      "Epoch 218 | Validation DotSimilarity is 0.8414679150540273\n",
      "Epoch 218 | Train Loss 0.0243238084790139\n",
      "\n",
      "Epoch 219 | Test DotSimilarity is 0.8418071008893239\n",
      "Epoch 219 | Validation DotSimilarity is 0.8409894772880251\n",
      "Epoch 219 | Train Loss 0.024237327940239074\n",
      "\n",
      "Epoch 220 | Test DotSimilarity is 0.8417771274015653\n",
      "Epoch 220 | Validation DotSimilarity is 0.840864516781772\n",
      "Epoch 220 | Train Loss 0.024088659633043144\n",
      "\n",
      "Epoch 221 | Test DotSimilarity is 0.8418035977219089\n",
      "Epoch 221 | Validation DotSimilarity is 0.8416235031066093\n",
      "Epoch 221 | Train Loss 0.024017601747377167\n",
      "\n",
      "Epoch 222 | Test DotSimilarity is 0.8428628920145907\n",
      "Epoch 222 | Validation DotSimilarity is 0.8418376671176191\n",
      "Epoch 222 | Train Loss 0.023893109254262062\n",
      "\n",
      "Epoch 223 | Test DotSimilarity is 0.8424375603362383\n",
      "Epoch 223 | Validation DotSimilarity is 0.8414871980129228\n",
      "Epoch 223 | Train Loss 0.02382901734680155\n",
      "\n",
      "Epoch 224 | Test DotSimilarity is 0.8441531692058355\n",
      "Epoch 224 | Validation DotSimilarity is 0.8432457990625273\n",
      "Epoch 224 | Train Loss 0.021579579801789192\n",
      "\n",
      "Epoch 225 | Test DotSimilarity is 0.843868387185949\n",
      "Epoch 225 | Validation DotSimilarity is 0.843414632062933\n",
      "Epoch 225 | Train Loss 0.020780170682806146\n",
      "\n",
      "Epoch 226 | Test DotSimilarity is 0.8438664733364818\n",
      "Epoch 226 | Validation DotSimilarity is 0.8434543120978444\n",
      "Epoch 226 | Train Loss 0.020564200290251765\n",
      "\n",
      "Epoch 227 | Test DotSimilarity is 0.8438110004597463\n",
      "Epoch 227 | Validation DotSimilarity is 0.8433152305478187\n",
      "Epoch 227 | Train Loss 0.02039550681506859\n",
      "\n",
      "Epoch 228 | Test DotSimilarity is 0.8435726080339011\n",
      "Epoch 228 | Validation DotSimilarity is 0.8432058709674722\n",
      "Epoch 228 | Train Loss 0.02027046122335934\n",
      "\n",
      "Epoch 229 | Test DotSimilarity is 0.8436256607655116\n",
      "Epoch 229 | Validation DotSimilarity is 0.8431606232026901\n",
      "Epoch 229 | Train Loss 0.020151689494078917\n",
      "\n",
      "Epoch 230 | Test DotSimilarity is 0.8438350341460072\n",
      "Epoch 230 | Validation DotSimilarity is 0.8433687122258325\n",
      "Epoch 230 | Train Loss 0.020053546400870686\n",
      "\n",
      "Epoch 231 | Test DotSimilarity is 0.8440207039883307\n",
      "Epoch 231 | Validation DotSimilarity is 0.8432074409709931\n",
      "Epoch 231 | Train Loss 0.019966283706643932\n",
      "\n",
      "Epoch 232 | Test DotSimilarity is 0.8441397946660926\n",
      "Epoch 232 | Validation DotSimilarity is 0.8434947515497264\n",
      "Epoch 232 | Train Loss 0.01992292371559599\n",
      "\n",
      "Epoch 233 | Test DotSimilarity is 0.8439934964194407\n",
      "Epoch 233 | Validation DotSimilarity is 0.8431986957039428\n",
      "Epoch 233 | Train Loss 0.01981777087950414\n",
      "\n",
      "Epoch 234 | Test DotSimilarity is 0.8437840327712518\n",
      "Epoch 234 | Validation DotSimilarity is 0.8434549340819384\n",
      "Epoch 234 | Train Loss 0.019749822850737606\n",
      "\n",
      "Epoch 235 | Test DotSimilarity is 0.8438056007781944\n",
      "Epoch 235 | Validation DotSimilarity is 0.8432346718400746\n",
      "Epoch 235 | Train Loss 0.019698759142094666\n",
      "\n",
      "Epoch 236 | Test DotSimilarity is 0.8441267025772023\n",
      "Epoch 236 | Validation DotSimilarity is 0.8430289378142306\n",
      "Epoch 236 | Train Loss 0.01963493435517966\n",
      "\n",
      "Epoch 237 | Test DotSimilarity is 0.8436453045559076\n",
      "Epoch 237 | Validation DotSimilarity is 0.8434888570886742\n",
      "Epoch 237 | Train Loss 0.01959928883100379\n",
      "\n",
      "Epoch 238 | Test DotSimilarity is 0.8439857882389479\n",
      "Epoch 238 | Validation DotSimilarity is 0.84271966498485\n",
      "Epoch 238 | Train Loss 0.01952058087621338\n",
      "\n",
      "Epoch 239 | Test DotSimilarity is 0.8438541072557564\n",
      "Epoch 239 | Validation DotSimilarity is 0.8433933409122532\n",
      "Epoch 239 | Train Loss 0.019452245390944926\n",
      "\n",
      "Epoch 240 | Test DotSimilarity is 0.8432788145969636\n",
      "Epoch 240 | Validation DotSimilarity is 0.842556269688216\n",
      "Epoch 240 | Train Loss 0.01941362102973103\n",
      "\n",
      "Epoch 241 | Test DotSimilarity is 0.8432996409948271\n",
      "Epoch 241 | Validation DotSimilarity is 0.8430886538935533\n",
      "Epoch 241 | Train Loss 0.019371554538666614\n",
      "\n",
      "Epoch 242 | Test DotSimilarity is 0.8438251152088849\n",
      "Epoch 242 | Validation DotSimilarity is 0.843015063319541\n",
      "Epoch 242 | Train Loss 0.01933528146064073\n",
      "\n",
      "Epoch 243 | Test DotSimilarity is 0.8441281814409555\n",
      "Epoch 243 | Validation DotSimilarity is 0.8432137192054648\n",
      "Epoch 243 | Train Loss 0.019271501413813473\n",
      "\n",
      "Epoch 244 | Test DotSimilarity is 0.8442813867211107\n",
      "Epoch 244 | Validation DotSimilarity is 0.8429354628361961\n",
      "Epoch 244 | Train Loss 0.019238356145098265\n",
      "\n",
      "Epoch 245 | Test DotSimilarity is 0.8430267439583827\n",
      "Epoch 245 | Validation DotSimilarity is 0.8427600055720119\n",
      "Epoch 245 | Train Loss 0.01920421394724962\n",
      "\n",
      "Epoch 246 | Test DotSimilarity is 0.8437379109237024\n",
      "Epoch 246 | Validation DotSimilarity is 0.8430979298877697\n",
      "Epoch 246 | Train Loss 0.01917064195191231\n",
      "\n",
      "Epoch 247 | Test DotSimilarity is 0.8438287894017842\n",
      "Epoch 247 | Validation DotSimilarity is 0.8429166763281996\n",
      "Epoch 247 | Train Loss 0.019131135200800686\n",
      "\n",
      "Epoch 248 | Test DotSimilarity is 0.8435115368987354\n",
      "Epoch 248 | Validation DotSimilarity is 0.8429320526609947\n",
      "Epoch 248 | Train Loss 0.019081394825830177\n",
      "\n",
      "Epoch 249 | Test DotSimilarity is 0.8437292760496701\n",
      "Epoch 249 | Validation DotSimilarity is 0.8430735188702309\n",
      "Epoch 249 | Train Loss 0.019038717307925123\n",
      "\n",
      "Epoch 250 | Test DotSimilarity is 0.8433940384394624\n",
      "Epoch 250 | Validation DotSimilarity is 0.8434282640230624\n",
      "Epoch 250 | Train Loss 0.0190156034764251\n",
      "\n",
      "Epoch 251 | Test DotSimilarity is 0.8433917220578383\n",
      "Epoch 251 | Validation DotSimilarity is 0.8424000519489189\n",
      "Epoch 251 | Train Loss 0.018992125792203985\n",
      "\n",
      "Epoch 252 | Test DotSimilarity is 0.8438140188853865\n",
      "Epoch 252 | Validation DotSimilarity is 0.8432228944002039\n",
      "Epoch 252 | Train Loss 0.018942184389405564\n",
      "\n",
      "Epoch 253 | Test DotSimilarity is 0.8438404253909961\n",
      "Epoch 253 | Validation DotSimilarity is 0.8431895571307617\n",
      "Epoch 253 | Train Loss 0.018900318665269998\n",
      "\n",
      "Epoch 254 | Test DotSimilarity is 0.8437233264797789\n",
      "Epoch 254 | Validation DotSimilarity is 0.842880935632973\n",
      "Epoch 254 | Train Loss 0.01886641181304556\n",
      "\n",
      "Epoch 255 | Test DotSimilarity is 0.8435920193336522\n",
      "Epoch 255 | Validation DotSimilarity is 0.8433308935627217\n",
      "Epoch 255 | Train Loss 0.018832476002990033\n",
      "\n",
      "Epoch 256 | Test DotSimilarity is 0.8430930484048187\n",
      "Epoch 256 | Validation DotSimilarity is 0.8429939090189978\n",
      "Epoch 256 | Train Loss 0.018799590507395048\n",
      "\n",
      "Epoch 257 | Test DotSimilarity is 0.8437796430824998\n",
      "Epoch 257 | Validation DotSimilarity is 0.8427779997765138\n",
      "Epoch 257 | Train Loss 0.018776304534736978\n",
      "\n",
      "Epoch 258 | Test DotSimilarity is 0.8437310209038391\n",
      "Epoch 258 | Validation DotSimilarity is 0.8428459179070938\n",
      "Epoch 258 | Train Loss 0.018752063671638022\n",
      "\n",
      "Epoch 259 | Test DotSimilarity is 0.8438914199751205\n",
      "Epoch 259 | Validation DotSimilarity is 0.8437161342346171\n",
      "Epoch 259 | Train Loss 0.017881739357562702\n",
      "\n",
      "Epoch 260 | Test DotSimilarity is 0.8443442081929806\n",
      "Epoch 260 | Validation DotSimilarity is 0.843636841287409\n",
      "Epoch 260 | Train Loss 0.01759899763021272\n",
      "\n",
      "Epoch 261 | Test DotSimilarity is 0.8443801918508932\n",
      "Epoch 261 | Validation DotSimilarity is 0.8437251585268783\n",
      "Epoch 261 | Train Loss 0.017504911343994727\n",
      "\n",
      "Epoch 262 | Test DotSimilarity is 0.8441211819375267\n",
      "Epoch 262 | Validation DotSimilarity is 0.8437832873100067\n",
      "Epoch 262 | Train Loss 0.017432469560635416\n",
      "\n",
      "Epoch 263 | Test DotSimilarity is 0.844174638470223\n",
      "Epoch 263 | Validation DotSimilarity is 0.8435960286663536\n",
      "Epoch 263 | Train Loss 0.01738522219124147\n",
      "\n",
      "Epoch 264 | Test DotSimilarity is 0.8442338301740616\n",
      "Epoch 264 | Validation DotSimilarity is 0.8438436711153298\n",
      "Epoch 264 | Train Loss 0.017337130159302494\n",
      "\n",
      "Epoch 265 | Test DotSimilarity is 0.8438995955587525\n",
      "Epoch 265 | Validation DotSimilarity is 0.8436098167868545\n",
      "Epoch 265 | Train Loss 0.01730728126654128\n",
      "\n",
      "Epoch 266 | Test DotSimilarity is 0.843877806185694\n",
      "Epoch 266 | Validation DotSimilarity is 0.8434621876822124\n",
      "Epoch 266 | Train Loss 0.01727367765374078\n",
      "\n",
      "Epoch 267 | Test DotSimilarity is 0.8442934009357778\n",
      "Epoch 267 | Validation DotSimilarity is 0.8436576397847795\n",
      "Epoch 267 | Train Loss 0.017235612122287917\n",
      "\n",
      "Epoch 268 | Test DotSimilarity is 0.8442633761565906\n",
      "Epoch 268 | Validation DotSimilarity is 0.8433220274178616\n",
      "Epoch 268 | Train Loss 0.01721870105411326\n",
      "\n",
      "Epoch 269 | Test DotSimilarity is 0.8439782062801718\n",
      "Epoch 269 | Validation DotSimilarity is 0.8438823435202172\n",
      "Epoch 269 | Train Loss 0.017190551406095538\n",
      "\n",
      "Epoch 270 | Test DotSimilarity is 0.8442185800395738\n",
      "Epoch 270 | Validation DotSimilarity is 0.8432371839205774\n",
      "Epoch 270 | Train Loss 0.017164145181381808\n",
      "\n",
      "Epoch 271 | Test DotSimilarity is 0.844260718577309\n",
      "Epoch 271 | Validation DotSimilarity is 0.8435607865271417\n",
      "Epoch 271 | Train Loss 0.01712783506686693\n",
      "\n",
      "Epoch 272 | Test DotSimilarity is 0.8442556581777393\n",
      "Epoch 272 | Validation DotSimilarity is 0.843731469368103\n",
      "Epoch 272 | Train Loss 0.01711156883733558\n",
      "\n",
      "Epoch 273 | Test DotSimilarity is 0.8441083075131528\n",
      "Epoch 273 | Validation DotSimilarity is 0.8435685910164779\n",
      "Epoch 273 | Train Loss 0.017084489084524184\n",
      "\n",
      "Epoch 274 | Test DotSimilarity is 0.8442108548862282\n",
      "Epoch 274 | Validation DotSimilarity is 0.8434200867419864\n",
      "Epoch 274 | Train Loss 0.01705545351637136\n",
      "\n",
      "Epoch 275 | Test DotSimilarity is 0.8437486400740986\n",
      "Epoch 275 | Validation DotSimilarity is 0.8434212384571005\n",
      "Epoch 275 | Train Loss 0.017037585666335633\n",
      "\n",
      "Epoch 276 | Test DotSimilarity is 0.8440154505161412\n",
      "Epoch 276 | Validation DotSimilarity is 0.8432310487872524\n",
      "Epoch 276 | Train Loss 0.01701344674030342\n",
      "\n",
      "Epoch 277 | Test DotSimilarity is 0.8440807676791184\n",
      "Epoch 277 | Validation DotSimilarity is 0.8435387905994683\n",
      "Epoch 277 | Train Loss 0.01700714777386751\n",
      "\n",
      "Epoch 278 | Test DotSimilarity is 0.8439448812082695\n",
      "Epoch 278 | Validation DotSimilarity is 0.8435440432933139\n",
      "Epoch 278 | Train Loss 0.01698157842094532\n",
      "\n",
      "Epoch 279 | Test DotSimilarity is 0.8444071814151551\n",
      "Epoch 279 | Validation DotSimilarity is 0.8435327154994858\n",
      "Epoch 279 | Train Loss 0.016956143548077146\n",
      "\n",
      "Epoch 280 | Test DotSimilarity is 0.8440910087668037\n",
      "Epoch 280 | Validation DotSimilarity is 0.8437268695441434\n",
      "Epoch 280 | Train Loss 0.016953472043537008\n",
      "\n",
      "Epoch 281 | Test DotSimilarity is 0.8442358407541979\n",
      "Epoch 281 | Validation DotSimilarity is 0.8434334162657219\n",
      "Epoch 281 | Train Loss 0.01692569375804018\n",
      "\n",
      "Epoch 282 | Test DotSimilarity is 0.8438637477096453\n",
      "Epoch 282 | Validation DotSimilarity is 0.8430682437939025\n",
      "Epoch 282 | Train Loss 0.016911421509341877\n",
      "\n",
      "Epoch 283 | Test DotSimilarity is 0.8440367408456229\n",
      "Epoch 283 | Validation DotSimilarity is 0.8431738855194711\n",
      "Epoch 283 | Train Loss 0.016884996514886286\n",
      "\n",
      "Epoch 284 | Test DotSimilarity is 0.8437156941892965\n",
      "Epoch 284 | Validation DotSimilarity is 0.8434734203243375\n",
      "Epoch 284 | Train Loss 0.01687229526406741\n",
      "\n",
      "Epoch 285 | Test DotSimilarity is 0.8440521550000943\n",
      "Epoch 285 | Validation DotSimilarity is 0.8434414924579895\n",
      "Epoch 285 | Train Loss 0.016867555724943722\n",
      "\n",
      "Epoch 286 | Test DotSimilarity is 0.8441780524781519\n",
      "Epoch 286 | Validation DotSimilarity is 0.8441606366413542\n",
      "Epoch 286 | Train Loss 0.016839322355961174\n",
      "\n",
      "Epoch 287 | Test DotSimilarity is 0.8440540093331894\n",
      "Epoch 287 | Validation DotSimilarity is 0.8435321476921641\n",
      "Epoch 287 | Train Loss 0.01681654617141678\n",
      "\n",
      "Epoch 288 | Test DotSimilarity is 0.8443422391666326\n",
      "Epoch 288 | Validation DotSimilarity is 0.8433983737362872\n",
      "Epoch 288 | Train Loss 0.016817012494351267\n",
      "\n",
      "Epoch 289 | Test DotSimilarity is 0.8445655299970771\n",
      "Epoch 289 | Validation DotSimilarity is 0.8433892063807827\n",
      "Epoch 289 | Train Loss 0.016788478503836128\n",
      "\n",
      "Epoch 290 | Test DotSimilarity is 0.8441408917337754\n",
      "Epoch 290 | Validation DotSimilarity is 0.8433816399144584\n",
      "Epoch 290 | Train Loss 0.016770293047826568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "######################################\n",
    "#  LOSS\n",
    "######################################\n",
    "\n",
    "# Root mean squared error\n",
    "loss_fn = torch.nn.HuberLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)  \n",
    "scheduler = StepLR(optimizer, step_size=35, gamma=0.5)\n",
    "\n",
    "# Use GPU for training\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap data in a data loader\n",
    "\n",
    "\n",
    "\n",
    "NUM_GRAPHS_PER_BATCH = 64\n",
    "\n",
    "\n",
    "loader = DataLoader(train_dataset, \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "SAVE_EVERY_X_EPOCH = 10\n",
    "REPORT_EVERY_X_EPOCH = 1\n",
    "\n",
    "def train(loader):\n",
    "    # Enumerate over the data\n",
    "    loss_per_batch = np.array([])\n",
    "    for batch in loader:\n",
    "        # Use GPU\n",
    "        batch.to(device)  \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Passing the node features and the connection info\n",
    "        pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch) \n",
    "        # Calculating the loss and gradients\n",
    "        loss = loss_fn(pred, batch.y)\n",
    "        loss.backward()  \n",
    "        # Update using the gradients\n",
    "        optimizer.step()\n",
    "        loss_per_batch = np.concatenate((loss_per_batch, np.array([loss.clone().detach().cpu().numpy()])))\n",
    "    return loss_per_batch\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(190, 291):\n",
    "    scheduler.step()\n",
    "    loss = train(loader)\n",
    "    pred_test_similarity = np.array([])\n",
    "    pred_validation_similarity = np.array([])\n",
    "   \n",
    "   \n",
    "    if epoch % REPORT_EVERY_X_EPOCH == 0:\n",
    "        for batch in test_loader:\n",
    "            batch.to(device)  \n",
    "            pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch)\n",
    "\n",
    "            batch_similarity = validate_similarities(batch.y.detach().cpu().numpy(),\n",
    "                                  pred.detach().cpu().numpy(),\n",
    "                                  mass_pow=1.0, intensity_pow=0.5)\n",
    "\n",
    "            pred_test_similarity = np.concatenate((pred_test_similarity, batch_similarity))\n",
    "    \n",
    "        for batch in validation_loader:\n",
    "           \n",
    "            batch.to(device)  \n",
    "            pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch)\n",
    "\n",
    "            batch_similarity = validate_similarities(batch.y.detach().cpu().numpy(),\n",
    "                                  pred.detach().cpu().numpy(),\n",
    "                                  mass_pow=1.0, intensity_pow=0.5)\n",
    "\n",
    "            pred_validation_similarity = np.concatenate((pred_validation_similarity, batch_similarity))\n",
    "            \n",
    "        \n",
    "        print(f\"Epoch {epoch} | Test DotSimilarity is {pred_test_similarity.mean()}\")\n",
    "        print(f\"Epoch {epoch} | Validation DotSimilarity is {pred_validation_similarity.mean()}\")\n",
    "        print(f\"Epoch {epoch} | Train Loss {loss.mean()}\")\n",
    "        print()\n",
    "\n",
    "    \n",
    "    if epoch % SAVE_EVERY_X_EPOCH == 0:\n",
    "        SAVE_PATH = f\"{epoch}.pt\"\n",
    "            \n",
    "        # Save model\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'metadata': {\"loss\" : \"HuberLoss\",\n",
    "                     \"Dataset\": \"Preprocessed_test_log_preparation_no_sparse_small\",\n",
    "                     \"test_similarities\": pred_test_similarity.mean()}\n",
    "        }, os.path.join(MODEL_SAVE, SAVE_PATH))\n",
    "\n",
    "        LOSS_FILE = f\"all_loss_until_{epoch}.output\"\n",
    "        with open(os.path.join(MODEL_SAVE, LOSS_FILE), 'wb') as fid:\n",
    "            pickle.dump(loss.mean(), fid)\n",
    "            fid.close() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCHRghd9lQtA"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
