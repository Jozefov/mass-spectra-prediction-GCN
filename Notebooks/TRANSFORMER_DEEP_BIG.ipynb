{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50m170L8kmQC",
    "outputId": "c817cc70-c5a9-4b85-8baa-c45186e45d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting matchms\n",
      "  Downloading matchms-0.18.0-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from matchms) (3.7.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from matchms) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from matchms) (4.65.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from matchms) (3.1)\n",
      "Requirement already satisfied: numba>=0.47 in /usr/local/lib/python3.10/dist-packages (from matchms) (0.56.4)\n",
      "Collecting pyteomics>=4.2\n",
      "  Downloading pyteomics-4.6-py2.py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.1/235.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from matchms) (2.27.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from matchms) (1.22.4)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from matchms) (4.9.2)\n",
      "Collecting pickydict>=0.4.0\n",
      "  Downloading pickydict-0.4.0-py3-none-any.whl (6.1 kB)\n",
      "Collecting sparsestack>=0.4.1\n",
      "  Downloading sparsestack-0.4.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.47->matchms) (67.7.2)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.47->matchms) (0.39.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->matchms) (1.14.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (4.39.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matchms) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->matchms) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->matchms) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->matchms) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->matchms) (1.26.15)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->matchms) (1.16.0)\n",
      "Installing collected packages: pyteomics, pickydict, deprecated, sparsestack, matchms\n",
      "Successfully installed deprecated-1.2.13 matchms-0.18.0 pickydict-0.4.0 pyteomics-4.6 sparsestack-0.4.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting rdkit\n",
      "  Downloading rdkit-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (8.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n",
      "Installing collected packages: rdkit\n",
      "Successfully installed rdkit-2022.9.5\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
      "Building wheels for collected packages: torch_geometric\n",
      "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910476 sha256=b7ed7c647db1c98e56a950593f60db8aae2fd5b4b3b389d617505e8470766315\n",
      "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
      "Successfully built torch_geometric\n",
      "Installing collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.3.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=256439 sha256=65da3e9f07cfd3b25cdb51e104c7fc413fa827854c9ba306fd5a2f646beea513\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install matchms\n",
    "!pip install rdkit\n",
    "!pip install torch_geometric\n",
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDErmuSHlP-z"
   },
   "outputs": [],
   "source": [
    "from matchms.importing import load_from_msp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import matchms\n",
    "from matchms import Spectrum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool, GATConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3oU8Vn3lQBM",
    "outputId": "e4ac4b61-a358-4272-caae-3bb06b16358a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbbhD8VUlQD2"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/MyDrive/NIST_SMALL\")\n",
    "BASE_DIRECTORY = \"/content/drive/MyDrive/NIST_SMALL\"\n",
    "\n",
    "TEST_DATA_SIZE = 5000\n",
    "OUTPUT_SIZE = 1000\n",
    "INTENSITY_POWER = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQHXkp8olQG9"
   },
   "outputs": [],
   "source": [
    "# with open(\"/content/drive/MyDrive/NIST_SMALL/Preprocessed_test_log_preparation_no_sparse_small.output\", 'rb') as handle:\n",
    "#    data_list_test  = pickle.load(handle)\n",
    "\n",
    "# with open(\"/content/drive/MyDrive/NIST_SMALL/Preprocessed_train_log_preparation_no_sparse_small.output\", 'rb') as handle:\n",
    "#    data_list_train  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ien3jRsYjaY8"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/NIST_SMALL/Preprocessed_test_pow_preparation_no_sparse_small.output\", 'rb') as handle:\n",
    "    data_list_test  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygTxmHh0juJv"
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_VALIDATION = 5000\n",
    "# shuffled_indices = np.random.permutation(len(data_list_train))\n",
    "# train_indices = shuffled_indices[NUMBER_OF_VALIDATION:]\n",
    "# val_indices = shuffled_indices[:NUMBER_OF_VALIDATION]\n",
    "\n",
    "test_dataset = data_list_test[:NUMBER_OF_VALIDATION]\n",
    "\n",
    "# validation_dataset = torch.utils.data.Subset(data_list_train, val_indices)\n",
    "# train_dataset = torch.utils.data.Subset(data_list_train, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ued52HQujwcq"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/NIST_SMALL/train_subset_pow.pkl\", 'rb') as handle:\n",
    "    train_dataset  = pickle.load(handle)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/NIST_SMALL/validation_subset_pow.pkl\", 'rb') as handle:\n",
    "    validation_dataset  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US04fy7IlQLM"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 2000\n",
    "NODE_FEATURES = 50\n",
    "EDGE_EMBEDDING = 10\n",
    "MASS_SHIFT = 5 \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NO7eAbp0lQNe"
   },
   "outputs": [],
   "source": [
    "def mask_prediction_by_mass(total_mass, raw_prediction, index_shift):\n",
    "    # Zero out predictions to the right of the maximum possible mass.\n",
    "    # input \n",
    "    # anchor_indices: shape (,batch_size) = ex [3,4,5]\n",
    "    #     total_mass = Weights of whole molecule, not only fragment\n",
    "    # data: shape (batch_size, embedding), embedding from GNN in our case\n",
    "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
    "    # \n",
    "\n",
    "    data = raw_prediction.type(torch.float64)\n",
    "    \n",
    "    total_mass = torch.round(total_mass).type(torch.int64)\n",
    "    indices = torch.arange(data.shape[-1])[None, ...].to(device)\n",
    "\n",
    "    right_of_total_mass = indices > (\n",
    "            total_mass[..., None] +\n",
    "            index_shift)\n",
    "    return torch.where(right_of_total_mass, torch.zeros_like(data),\n",
    "                        data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4N4vUlglQQw"
   },
   "outputs": [],
   "source": [
    "def scatter_by_anchor_indices(anchor_indices, data, index_shift):\n",
    "    # reverse vector by anchor_indices and rest set to zero\n",
    "    # input \n",
    "    # anchor_indices: shape (,batch_size) = ex [3,4,5]\n",
    "    #     total_mass = Weights of whole molecule, not only fragment\n",
    "    # data: shape (batch_size, embedding), embedding from GNN in our case\n",
    "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
    "    \n",
    "    index_shift = index_shift\n",
    "    anchor_indices = anchor_indices\n",
    "    data = data.type(torch.float64)\n",
    "    batch_size = data.shape[0]\n",
    "    \n",
    "    num_data_columns = data.shape[-1]\n",
    "    indices = torch.arange(num_data_columns)[None, ...].to(device)\n",
    "    shifted_indices = anchor_indices[..., None] - indices + index_shift\n",
    "    valid_indices = shifted_indices >= 0\n",
    "\n",
    "   \n",
    "\n",
    "    batch_indices = torch.tile(\n",
    "          torch.arange(batch_size)[..., None], [1, num_data_columns]).to(device)\n",
    "    shifted_indices += batch_indices * num_data_columns\n",
    "\n",
    "    shifted_indices = torch.reshape(shifted_indices, [-1])\n",
    "    num_elements = data.shape[0] * data.shape[1]\n",
    "    row_indices = torch.arange(num_elements).to(device)\n",
    "    stacked_indices = torch.stack([row_indices, shifted_indices], axis=1)\n",
    "\n",
    "\n",
    "    lower_batch_boundaries = torch.reshape(batch_indices * num_data_columns, [-1])\n",
    "    upper_batch_boundaries = torch.reshape(((batch_indices + 1) * num_data_columns),\n",
    "                                          [-1])\n",
    "\n",
    "    valid_indices = torch.logical_and(shifted_indices >= lower_batch_boundaries,\n",
    "                                     shifted_indices < upper_batch_boundaries)\n",
    "\n",
    "    stacked_indices = stacked_indices[valid_indices]\n",
    "    \n",
    "\n",
    "    dense_shape = torch.tile(torch.tensor(num_elements)[..., None], [2]).type(torch.int32)\n",
    "\n",
    "    scattering_matrix = torch.sparse.FloatTensor(stacked_indices.type(torch.int64).T,\n",
    "                                                 torch.ones_like(stacked_indices[:, 0]).type(torch.float64),\n",
    "                                                dense_shape.tolist())\n",
    "\n",
    "    flattened_data = torch.reshape(data, [-1])[..., None]\n",
    "    flattened_output = torch.sparse.mm(scattering_matrix, flattened_data)\n",
    "    return torch.reshape(torch.transpose(flattened_output, 0, 1), [-1, num_data_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dln8v3AClQSS"
   },
   "outputs": [],
   "source": [
    "def reverse_prediction(total_mass, raw_prediction, index_shift):\n",
    "    # reverse vector by anchor_indices and rest set to zero and make preproessing\n",
    "    # input \n",
    "    # total_mass: shape (,batch_size) = ex [3,4,5]\n",
    "    #     total_mass = Weights of whole molecule, not only fragment\n",
    "    # raw_prediction: shape (batch_size, embedding), embedding from GNN in our case\n",
    "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
    "    #     total_mass = feature_dict[fmap_constants.MOLECULE_WEIGHT][..., 0]\n",
    "    \n",
    "    total_mass = torch.round(total_mass).type(torch.int32)\n",
    "    return scatter_by_anchor_indices(\n",
    "        total_mass, raw_prediction, index_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBA9SAJolQUN"
   },
   "outputs": [],
   "source": [
    "def dot_product(true, pred, mass_pow=3, intensity_pow=0.6):\n",
    "    # shape for true and pred is one dimensional array\n",
    "    # pred (number_of_predicted_bins)\n",
    "    # defaul value for mass_pow and intensity_pow is set for Stein dot product\n",
    "    assert true.ndim == pred.ndim and true.ndim == 1\n",
    "    length = true.shape[-1]\n",
    "    mass = np.arange(length).astype(np.float64)\n",
    "        \n",
    "    wl = mass ** mass_pow * pred**intensity_pow\n",
    "    wu = mass ** mass_pow * true**intensity_pow\n",
    "    \n",
    "    pred_weighted_norm = np.sqrt(np.sum((wl**2)))\n",
    "    true_weighted_norm = np.sqrt(np.sum((wu**2)))\n",
    "    \n",
    "    result = np.sum(wl*wu) / (pred_weighted_norm * true_weighted_norm)\n",
    " \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsDAQUwQlQWk"
   },
   "outputs": [],
   "source": [
    "def validate_similarities(true, pred, mass_pow, intensity_pow):\n",
    "    # Helper function for validation\n",
    "    similarities = np.array([])\n",
    "    for true_instance, pred_instance in zip(true, pred):\n",
    "        tmp = dot_product(true_instance, pred_instance, mass_pow=mass_pow, intensity_pow=intensity_pow)\n",
    "        \n",
    "        similarities = np.concatenate((similarities, tmp), axis=None)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uez-8A76lQa_"
   },
   "outputs": [],
   "source": [
    "class SKIPblock(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, USE_dropout=True, dropout_rate = 0.2):\n",
    "        super().__init__()\n",
    "        #only need to change shape of the residual if num_channels changes (i.e. in_c != out_c)\n",
    "        #[bs,in_c,seq_length]->conv(1,in_c,out_c)->[bs,out_c,seq_length]\n",
    "        \n",
    "        self.hidden1= nn.utils.weight_norm(nn.Linear(in_features, hidden_features),name='weight',dim=0)\n",
    "        if USE_dropout:\n",
    "            self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.hidden2 = nn.utils.weight_norm(nn.Linear(hidden_features, in_features),name='weight',dim=0)\n",
    "        if USE_dropout:\n",
    "            self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden = self.hidden1(x)\n",
    "        hidden = self.dropout1(hidden)\n",
    "        hidden = self.relu1(hidden)\n",
    "\n",
    "        hidden = self.hidden2(hidden)\n",
    "        hidden = hidden + x\n",
    "        hidden = self.relu2(hidden)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2dKIsoglQcs",
    "outputId": "817d923a-2de7-4a25-c5ee-54485aa740c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMER_CONV_MESSAGE_BIG(\n",
      "  (initial_conv): TransformerConv(50, 200, heads=4)\n",
      "  (conv1): TransformerConv(800, 200, heads=4)\n",
      "  (conv2): TransformerConv(800, 200, heads=4)\n",
      "  (conv3): TransformerConv(800, 200, heads=4)\n",
      "  (conv4): TransformerConv(800, 200, heads=4)\n",
      "  (bottleneck): Linear(in_features=200, out_features=2000, bias=True)\n",
      "  (skip1): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip2): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip3): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip4): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip5): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip6): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (skip7): SKIPblock(\n",
      "    (hidden1): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (relu1): ReLU()\n",
      "    (hidden2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (relu_out_resnet): ReLU()\n",
      "  (forward_prediction): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "  (backward_prediction): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "  (gate): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "  (relu_out): ReLU()\n",
      ")\n",
      "Number of parameters:  72446600\n"
     ]
    }
   ],
   "source": [
    "class TRANSFORMER_CONV_MESSAGE_BIG(torch.nn.Module):\n",
    "    def __init__(self, heads, dropout):\n",
    "        # Init parent\n",
    "        super(TRANSFORMER_CONV_MESSAGE_BIG, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        EMBEDDING_SIZE_REDUCED = int(EMBEDDING_SIZE*0.1)\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = TransformerConv(NODE_FEATURES, EMBEDDING_SIZE_REDUCED, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        self.conv1 = TransformerConv(EMBEDDING_SIZE_REDUCED*heads, EMBEDDING_SIZE_REDUCED, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        self.conv2 = TransformerConv(EMBEDDING_SIZE_REDUCED*heads, EMBEDDING_SIZE_REDUCED, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        self.conv3 = TransformerConv(EMBEDDING_SIZE_REDUCED*heads, EMBEDDING_SIZE_REDUCED, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        self.conv4 = TransformerConv(EMBEDDING_SIZE_REDUCED*heads, EMBEDDING_SIZE_REDUCED, concat=False, heads=heads, beta=True, dropout=dropout, edge_dim=EDGE_EMBEDDING)\n",
    "        \n",
    "        self.bottleneck = Linear(EMBEDDING_SIZE_REDUCED, EMBEDDING_SIZE)\n",
    "\n",
    "        self.skip1 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip2 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip3 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip4 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip5 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip6 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.skip7 = SKIPblock(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "        self.relu_out_resnet = nn.ReLU()\n",
    "\n",
    "        self.forward_prediction = Linear(EMBEDDING_SIZE, OUTPUT_SIZE)\n",
    "        self.backward_prediction = Linear(EMBEDDING_SIZE, OUTPUT_SIZE)\n",
    "        self.gate = Linear(EMBEDDING_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "        self.relu_out = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, total_mass, batch_index):\n",
    "        \n",
    "        hidden = self.initial_conv(x, edge_index, edge_weight)\n",
    "        hidden = F.relu(hidden)\n",
    "     \n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index, edge_weight)\n",
    "        hidden = F.relu(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index, edge_weight)\n",
    "        hidden = F.relu(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index, edge_weight)\n",
    "        hidden = F.relu(hidden)\n",
    "        hidden = self.conv4(hidden, edge_index, edge_weight)\n",
    "        \n",
    "      \n",
    "        hidden = gap(hidden, batch_index)\n",
    "        hidden = self.bottleneck(hidden)\n",
    "\n",
    "        hidden = self.skip1(hidden)\n",
    "        hidden = self.skip2(hidden)\n",
    "        hidden = self.skip3(hidden)\n",
    "        hidden = self.skip4(hidden)\n",
    "        hidden = self.skip5(hidden)\n",
    "        hidden = self.skip6(hidden)\n",
    "        hidden = self.skip7(hidden)\n",
    "        \n",
    "        hidden = self.relu_out_resnet(hidden)\n",
    "\n",
    "        # Bidirectional layer\n",
    "        # Forward prediction\n",
    "        forward_prediction_hidden = self.forward_prediction(hidden)\n",
    "        forward_prediction_hidden = mask_prediction_by_mass(total_mass, forward_prediction_hidden, MASS_SHIFT)\n",
    "        \n",
    "        # # Backward prediction\n",
    "        backward_prediction_hidden = self.backward_prediction(hidden)\n",
    "        backward_prediction_hidden = reverse_prediction(total_mass, backward_prediction_hidden, MASS_SHIFT)\n",
    "        \n",
    "        # # Gate\n",
    "        gate_hidden = self.gate(hidden)\n",
    "        gate_hidden = F.sigmoid(gate_hidden)\n",
    "\n",
    "        # # Apply a final (linear) classifier.\n",
    "        out = gate_hidden * forward_prediction_hidden + (1. - gate_hidden) * backward_prediction_hidden\n",
    "        out = self.relu_out(out)\n",
    "        \n",
    "        out = out.type(torch.float64)\n",
    "        return out\n",
    "\n",
    "heads = 4\n",
    "dropout = 0.1\n",
    "model = TRANSFORMER_CONV_MESSAGE_BIG(heads, dropout)\n",
    "MODEL_NAME = \"TRANSFORMER_CONV_MESSAGE_BIG_POW\"\n",
    "MODEL_SAVE = os.path.join(BASE_DIRECTORY, MODEL_NAME)\n",
    "os.makedirs(MODEL_SAVE, mode=0o777, exist_ok=True)\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjbzcPf1lQfZ",
    "outputId": "1da7c2f7-c91e-467d-ac94-b0abc95debe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 | Test DotSimilarity is 0.763147555865506\n",
      "Epoch 0 | Validation DotSimilarity is 0.7628311546732448\n",
      "Epoch 0 | Train Loss 0.0004727543195389215\n",
      "\n",
      "Epoch 1 | Test DotSimilarity is 0.7892757248235717\n",
      "Epoch 1 | Validation DotSimilarity is 0.7895821529579218\n",
      "Epoch 1 | Train Loss 0.000392899382346316\n",
      "\n",
      "Epoch 2 | Test DotSimilarity is 0.8057254113792304\n",
      "Epoch 2 | Validation DotSimilarity is 0.806004839242652\n",
      "Epoch 2 | Train Loss 0.0003604965866350077\n",
      "\n",
      "Epoch 3 | Test DotSimilarity is 0.8127264122361272\n",
      "Epoch 3 | Validation DotSimilarity is 0.8131049756843344\n",
      "Epoch 3 | Train Loss 0.0003399959321121596\n",
      "\n",
      "Epoch 4 | Test DotSimilarity is 0.8205465654722626\n",
      "Epoch 4 | Validation DotSimilarity is 0.8204107633493962\n",
      "Epoch 4 | Train Loss 0.0003275351155965237\n",
      "\n",
      "Epoch 5 | Test DotSimilarity is 0.8256365048397596\n",
      "Epoch 5 | Validation DotSimilarity is 0.8258806002680329\n",
      "Epoch 5 | Train Loss 0.00031328520054535024\n",
      "\n",
      "Epoch 6 | Test DotSimilarity is 0.8319696746080616\n",
      "Epoch 6 | Validation DotSimilarity is 0.832448628024774\n",
      "Epoch 6 | Train Loss 0.0003022935339398318\n",
      "\n",
      "Epoch 7 | Test DotSimilarity is 0.8356306946659888\n",
      "Epoch 7 | Validation DotSimilarity is 0.8358510027846631\n",
      "Epoch 7 | Train Loss 0.00029300368825129836\n",
      "\n",
      "Epoch 8 | Test DotSimilarity is 0.838724089321871\n",
      "Epoch 8 | Validation DotSimilarity is 0.8389026701591887\n",
      "Epoch 8 | Train Loss 0.0002862745837383503\n",
      "\n",
      "Epoch 9 | Test DotSimilarity is 0.8416009017878606\n",
      "Epoch 9 | Validation DotSimilarity is 0.8415861020409673\n",
      "Epoch 9 | Train Loss 0.00027845439366764636\n",
      "\n",
      "Epoch 10 | Test DotSimilarity is 0.8427132769038981\n",
      "Epoch 10 | Validation DotSimilarity is 0.8429645386384927\n",
      "Epoch 10 | Train Loss 0.00027180610933714614\n",
      "\n",
      "Epoch 11 | Test DotSimilarity is 0.8455546411208062\n",
      "Epoch 11 | Validation DotSimilarity is 0.845308997384266\n",
      "Epoch 11 | Train Loss 0.0002661644451835218\n",
      "\n",
      "Epoch 12 | Test DotSimilarity is 0.8467356528431672\n",
      "Epoch 12 | Validation DotSimilarity is 0.8468671042839716\n",
      "Epoch 12 | Train Loss 0.00026002347011887554\n",
      "\n",
      "Epoch 13 | Test DotSimilarity is 0.847783263721532\n",
      "Epoch 13 | Validation DotSimilarity is 0.8482078200496286\n",
      "Epoch 13 | Train Loss 0.0002551773987424725\n",
      "\n",
      "Epoch 14 | Test DotSimilarity is 0.8494478879901668\n",
      "Epoch 14 | Validation DotSimilarity is 0.8504471730666584\n",
      "Epoch 14 | Train Loss 0.0002496167017119147\n",
      "\n",
      "Epoch 15 | Test DotSimilarity is 0.8455959066076526\n",
      "Epoch 15 | Validation DotSimilarity is 0.8467480295961078\n",
      "Epoch 15 | Train Loss 0.00024542824440699693\n",
      "\n",
      "Epoch 16 | Test DotSimilarity is 0.8515280513147098\n",
      "Epoch 16 | Validation DotSimilarity is 0.8517258549820461\n",
      "Epoch 16 | Train Loss 0.00024248922452972286\n",
      "\n",
      "Epoch 17 | Test DotSimilarity is 0.850638060113287\n",
      "Epoch 17 | Validation DotSimilarity is 0.8514440819914587\n",
      "Epoch 17 | Train Loss 0.0002384960084066415\n",
      "\n",
      "Epoch 18 | Test DotSimilarity is 0.8513639649807218\n",
      "Epoch 18 | Validation DotSimilarity is 0.8525991969699739\n",
      "Epoch 18 | Train Loss 0.00023305607666680375\n",
      "\n",
      "Epoch 19 | Test DotSimilarity is 0.8502145526948891\n",
      "Epoch 19 | Validation DotSimilarity is 0.8508907649151235\n",
      "Epoch 19 | Train Loss 0.0002302100347853848\n",
      "\n",
      "Epoch 20 | Test DotSimilarity is 0.8524260945437981\n",
      "Epoch 20 | Validation DotSimilarity is 0.8531139376419694\n",
      "Epoch 20 | Train Loss 0.00022765176855532178\n",
      "\n",
      "Epoch 21 | Test DotSimilarity is 0.8514656291343088\n",
      "Epoch 21 | Validation DotSimilarity is 0.8520140548625103\n",
      "Epoch 21 | Train Loss 0.0002238690862865182\n",
      "\n",
      "Epoch 22 | Test DotSimilarity is 0.852139930153784\n",
      "Epoch 22 | Validation DotSimilarity is 0.8540323803089854\n",
      "Epoch 22 | Train Loss 0.00022112858694831565\n",
      "\n",
      "Epoch 23 | Test DotSimilarity is 0.8531095680807564\n",
      "Epoch 23 | Validation DotSimilarity is 0.8540040755076436\n",
      "Epoch 23 | Train Loss 0.00021679612658881135\n",
      "\n",
      "Epoch 24 | Test DotSimilarity is 0.8546402004066495\n",
      "Epoch 24 | Validation DotSimilarity is 0.855872555603236\n",
      "Epoch 24 | Train Loss 0.00021357120609228348\n",
      "\n",
      "Epoch 25 | Test DotSimilarity is 0.8537974789757327\n",
      "Epoch 25 | Validation DotSimilarity is 0.8549002259295532\n",
      "Epoch 25 | Train Loss 0.0002102045483319389\n",
      "\n",
      "Epoch 26 | Test DotSimilarity is 0.8537821540908274\n",
      "Epoch 26 | Validation DotSimilarity is 0.8550910399889287\n",
      "Epoch 26 | Train Loss 0.0002072217789419892\n",
      "\n",
      "Epoch 27 | Test DotSimilarity is 0.8537914517464966\n",
      "Epoch 27 | Validation DotSimilarity is 0.8555816306793785\n",
      "Epoch 27 | Train Loss 0.00020561651998690696\n",
      "\n",
      "Epoch 28 | Test DotSimilarity is 0.8542694169899332\n",
      "Epoch 28 | Validation DotSimilarity is 0.8554259719650256\n",
      "Epoch 28 | Train Loss 0.00020268608611375384\n",
      "\n",
      "Epoch 29 | Test DotSimilarity is 0.8549755300794499\n",
      "Epoch 29 | Validation DotSimilarity is 0.8554333307939909\n",
      "Epoch 29 | Train Loss 0.00019949886910923168\n",
      "\n",
      "Epoch 30 | Test DotSimilarity is 0.8536348332330715\n",
      "Epoch 30 | Validation DotSimilarity is 0.8547751295016145\n",
      "Epoch 30 | Train Loss 0.00019697402520368033\n",
      "\n",
      "Epoch 31 | Test DotSimilarity is 0.8544030645850808\n",
      "Epoch 31 | Validation DotSimilarity is 0.8555166514640048\n",
      "Epoch 31 | Train Loss 0.00019570669137925568\n",
      "\n",
      "Epoch 32 | Test DotSimilarity is 0.8550784175011482\n",
      "Epoch 32 | Validation DotSimilarity is 0.855879449447737\n",
      "Epoch 32 | Train Loss 0.00019209959983419237\n",
      "\n",
      "Epoch 33 | Test DotSimilarity is 0.8542137376187143\n",
      "Epoch 33 | Validation DotSimilarity is 0.8556720892939007\n",
      "Epoch 33 | Train Loss 0.00019137635546666475\n",
      "\n",
      "Epoch 34 | Test DotSimilarity is 0.8538840750292719\n",
      "Epoch 34 | Validation DotSimilarity is 0.8549527011686292\n",
      "Epoch 34 | Train Loss 0.0001887440043094349\n",
      "\n",
      "Epoch 35 | Test DotSimilarity is 0.8550511210593484\n",
      "Epoch 35 | Validation DotSimilarity is 0.8563532728302514\n",
      "Epoch 35 | Train Loss 0.00018567131340554923\n",
      "\n",
      "Epoch 36 | Test DotSimilarity is 0.8544849309911622\n",
      "Epoch 36 | Validation DotSimilarity is 0.8556523574605575\n",
      "Epoch 36 | Train Loss 0.00018531716112901587\n",
      "\n",
      "Epoch 37 | Test DotSimilarity is 0.853466156166493\n",
      "Epoch 37 | Validation DotSimilarity is 0.8549776386047111\n",
      "Epoch 37 | Train Loss 0.0001823650817482045\n",
      "\n",
      "Epoch 38 | Test DotSimilarity is 0.8544462856768452\n",
      "Epoch 38 | Validation DotSimilarity is 0.8559167722090336\n",
      "Epoch 38 | Train Loss 0.00018057519848198845\n",
      "\n",
      "Epoch 39 | Test DotSimilarity is 0.8550269332213284\n",
      "Epoch 39 | Validation DotSimilarity is 0.8562667899429376\n",
      "Epoch 39 | Train Loss 0.0001793464712637147\n",
      "\n",
      "Epoch 40 | Test DotSimilarity is 0.8550811435241871\n",
      "Epoch 40 | Validation DotSimilarity is 0.8558856892946555\n",
      "Epoch 40 | Train Loss 0.00017703624071755718\n",
      "\n",
      "Epoch 41 | Test DotSimilarity is 0.8546220035877735\n",
      "Epoch 41 | Validation DotSimilarity is 0.8558215187232058\n",
      "Epoch 41 | Train Loss 0.00017468920285601202\n",
      "\n",
      "Epoch 42 | Test DotSimilarity is 0.8543621905286723\n",
      "Epoch 42 | Validation DotSimilarity is 0.8555697801706105\n",
      "Epoch 42 | Train Loss 0.00017215173310848443\n",
      "\n",
      "Epoch 43 | Test DotSimilarity is 0.8542366790956654\n",
      "Epoch 43 | Validation DotSimilarity is 0.8552618354125238\n",
      "Epoch 43 | Train Loss 0.00017192645999061676\n",
      "\n",
      "Epoch 44 | Test DotSimilarity is 0.8546589140763498\n",
      "Epoch 44 | Validation DotSimilarity is 0.8565848884855659\n",
      "Epoch 44 | Train Loss 0.0001695184719796899\n",
      "\n",
      "Epoch 45 | Test DotSimilarity is 0.8555705607431507\n",
      "Epoch 45 | Validation DotSimilarity is 0.8565079578429381\n",
      "Epoch 45 | Train Loss 0.00016828187680868615\n",
      "\n",
      "Epoch 46 | Test DotSimilarity is 0.8542989234314741\n",
      "Epoch 46 | Validation DotSimilarity is 0.8554895271545349\n",
      "Epoch 46 | Train Loss 0.00016781203073561776\n",
      "\n",
      "Epoch 47 | Test DotSimilarity is 0.8545940409429708\n",
      "Epoch 47 | Validation DotSimilarity is 0.8556385555660243\n",
      "Epoch 47 | Train Loss 0.00016540188890568346\n",
      "\n",
      "Epoch 48 | Test DotSimilarity is 0.8544597092286491\n",
      "Epoch 48 | Validation DotSimilarity is 0.8563415316730923\n",
      "Epoch 48 | Train Loss 0.00016348476338833996\n",
      "\n",
      "Epoch 49 | Test DotSimilarity is 0.8544771695458736\n",
      "Epoch 49 | Validation DotSimilarity is 0.8560341874042396\n",
      "Epoch 49 | Train Loss 0.00016248485608409412\n",
      "\n",
      "Epoch 50 | Test DotSimilarity is 0.8555100118856938\n",
      "Epoch 50 | Validation DotSimilarity is 0.8568724605327154\n",
      "Epoch 50 | Train Loss 0.00016040654550240077\n",
      "\n",
      "Epoch 51 | Test DotSimilarity is 0.8547579737217172\n",
      "Epoch 51 | Validation DotSimilarity is 0.8567522839044861\n",
      "Epoch 51 | Train Loss 0.00015928006904956672\n",
      "\n",
      "Epoch 52 | Test DotSimilarity is 0.8548524857187821\n",
      "Epoch 52 | Validation DotSimilarity is 0.8565500859942459\n",
      "Epoch 52 | Train Loss 0.00015775004697001738\n",
      "\n",
      "Epoch 53 | Test DotSimilarity is 0.8554454836291169\n",
      "Epoch 53 | Validation DotSimilarity is 0.8568650483511429\n",
      "Epoch 53 | Train Loss 0.0001566344389541924\n",
      "\n",
      "Epoch 54 | Test DotSimilarity is 0.8546939146161916\n",
      "Epoch 54 | Validation DotSimilarity is 0.856057860708648\n",
      "Epoch 54 | Train Loss 0.00015528296326608137\n",
      "\n",
      "Epoch 55 | Test DotSimilarity is 0.8539517260003344\n",
      "Epoch 55 | Validation DotSimilarity is 0.8564680437138253\n",
      "Epoch 55 | Train Loss 0.00015460008174510964\n",
      "\n",
      "Epoch 56 | Test DotSimilarity is 0.8552369844202509\n",
      "Epoch 56 | Validation DotSimilarity is 0.8565767273693655\n",
      "Epoch 56 | Train Loss 0.00015280918324836904\n",
      "\n",
      "Epoch 57 | Test DotSimilarity is 0.8550184256903262\n",
      "Epoch 57 | Validation DotSimilarity is 0.8569151838557718\n",
      "Epoch 57 | Train Loss 0.00015118392334481951\n",
      "\n",
      "Epoch 58 | Test DotSimilarity is 0.8543956988558824\n",
      "Epoch 58 | Validation DotSimilarity is 0.8560683214135865\n",
      "Epoch 58 | Train Loss 0.00014994111160046187\n",
      "\n",
      "Epoch 59 | Test DotSimilarity is 0.8541092219428625\n",
      "Epoch 59 | Validation DotSimilarity is 0.8551469331366564\n",
      "Epoch 59 | Train Loss 0.0001496581232415293\n",
      "\n",
      "Epoch 60 | Test DotSimilarity is 0.85443905671969\n",
      "Epoch 60 | Validation DotSimilarity is 0.8565426269270753\n",
      "Epoch 60 | Train Loss 0.00014828600670562195\n",
      "\n",
      "Epoch 61 | Test DotSimilarity is 0.8547029735254109\n",
      "Epoch 61 | Validation DotSimilarity is 0.8566027989987378\n",
      "Epoch 61 | Train Loss 0.0001473595362044483\n",
      "\n",
      "Epoch 62 | Test DotSimilarity is 0.8548558213935455\n",
      "Epoch 62 | Validation DotSimilarity is 0.8568085716214697\n",
      "Epoch 62 | Train Loss 0.000146115476597722\n",
      "\n",
      "Epoch 63 | Test DotSimilarity is 0.8544424717678616\n",
      "Epoch 63 | Validation DotSimilarity is 0.8560752118398101\n",
      "Epoch 63 | Train Loss 0.0001446277175821969\n",
      "\n",
      "Epoch 64 | Test DotSimilarity is 0.8550977605788059\n",
      "Epoch 64 | Validation DotSimilarity is 0.8565644649583668\n",
      "Epoch 64 | Train Loss 0.00014326850208201347\n",
      "\n",
      "Epoch 65 | Test DotSimilarity is 0.8553747550563472\n",
      "Epoch 65 | Validation DotSimilarity is 0.856770017441464\n",
      "Epoch 65 | Train Loss 0.0001432093959356042\n",
      "\n",
      "Epoch 66 | Test DotSimilarity is 0.854256068257225\n",
      "Epoch 66 | Validation DotSimilarity is 0.855428253648225\n",
      "Epoch 66 | Train Loss 0.00014174592433545546\n",
      "\n",
      "Epoch 67 | Test DotSimilarity is 0.8553685825035149\n",
      "Epoch 67 | Validation DotSimilarity is 0.8563943257679344\n",
      "Epoch 67 | Train Loss 0.0001408640036814007\n",
      "\n",
      "Epoch 68 | Test DotSimilarity is 0.8532599115068722\n",
      "Epoch 68 | Validation DotSimilarity is 0.8543311319631601\n",
      "Epoch 68 | Train Loss 0.00013961998382772744\n",
      "\n",
      "Epoch 69 | Test DotSimilarity is 0.8553119908001788\n",
      "Epoch 69 | Validation DotSimilarity is 0.8565805637212861\n",
      "Epoch 69 | Train Loss 0.00013923976116103173\n",
      "\n",
      "Epoch 70 | Test DotSimilarity is 0.8552304120590097\n",
      "Epoch 70 | Validation DotSimilarity is 0.8563119864690397\n",
      "Epoch 70 | Train Loss 0.0001379277635880836\n",
      "\n",
      "Epoch 71 | Test DotSimilarity is 0.8551733978950712\n",
      "Epoch 71 | Validation DotSimilarity is 0.8557009114681531\n",
      "Epoch 71 | Train Loss 0.00013706804461988996\n",
      "\n",
      "Epoch 72 | Test DotSimilarity is 0.8564890854078919\n",
      "Epoch 72 | Validation DotSimilarity is 0.8574906592039079\n",
      "Epoch 72 | Train Loss 0.00013668658605421954\n",
      "\n",
      "Epoch 73 | Test DotSimilarity is 0.854803192113305\n",
      "Epoch 73 | Validation DotSimilarity is 0.8562381176005416\n",
      "Epoch 73 | Train Loss 0.00013632474618780217\n",
      "\n",
      "Epoch 74 | Test DotSimilarity is 0.8562547507352294\n",
      "Epoch 74 | Validation DotSimilarity is 0.8576523260100805\n",
      "Epoch 74 | Train Loss 0.00013597368167536734\n",
      "\n",
      "Epoch 75 | Test DotSimilarity is 0.8544239045777727\n",
      "Epoch 75 | Validation DotSimilarity is 0.8560397537756231\n",
      "Epoch 75 | Train Loss 0.00013404986788071637\n",
      "\n",
      "Epoch 76 | Test DotSimilarity is 0.8546295557940983\n",
      "Epoch 76 | Validation DotSimilarity is 0.8557647850286884\n",
      "Epoch 76 | Train Loss 0.0001345033999968765\n",
      "\n",
      "Epoch 77 | Test DotSimilarity is 0.8557871066459567\n",
      "Epoch 77 | Validation DotSimilarity is 0.8573184742588686\n",
      "Epoch 77 | Train Loss 0.0001329375482179503\n",
      "\n",
      "Epoch 78 | Test DotSimilarity is 0.8551994835310459\n",
      "Epoch 78 | Validation DotSimilarity is 0.8561010995601378\n",
      "Epoch 78 | Train Loss 0.0001323652877071069\n",
      "\n",
      "Epoch 79 | Test DotSimilarity is 0.8540466411716737\n",
      "Epoch 79 | Validation DotSimilarity is 0.8550659665762543\n",
      "Epoch 79 | Train Loss 0.00013173448891150592\n",
      "\n",
      "Epoch 80 | Test DotSimilarity is 0.8546188282968545\n",
      "Epoch 80 | Validation DotSimilarity is 0.8560549647334997\n",
      "Epoch 80 | Train Loss 0.0001308652594922613\n",
      "\n",
      "Epoch 81 | Test DotSimilarity is 0.8555093165922856\n",
      "Epoch 81 | Validation DotSimilarity is 0.8557607407030733\n",
      "Epoch 81 | Train Loss 0.00013031009340080772\n",
      "\n",
      "Epoch 82 | Test DotSimilarity is 0.8559176052089695\n",
      "Epoch 82 | Validation DotSimilarity is 0.8569128923697473\n",
      "Epoch 82 | Train Loss 0.0001291777325344393\n",
      "\n",
      "Epoch 83 | Test DotSimilarity is 0.8547014147032435\n",
      "Epoch 83 | Validation DotSimilarity is 0.8557972602946909\n",
      "Epoch 83 | Train Loss 0.00013086416322792167\n",
      "\n",
      "Epoch 84 | Test DotSimilarity is 0.8554090385163734\n",
      "Epoch 84 | Validation DotSimilarity is 0.856144729584522\n",
      "Epoch 84 | Train Loss 0.00012805549820001105\n",
      "\n",
      "Epoch 85 | Test DotSimilarity is 0.8547470622849156\n",
      "Epoch 85 | Validation DotSimilarity is 0.8548242972493972\n",
      "Epoch 85 | Train Loss 0.00012727874073803952\n",
      "\n",
      "Epoch 86 | Test DotSimilarity is 0.8557567111162513\n",
      "Epoch 86 | Validation DotSimilarity is 0.856946122159978\n",
      "Epoch 86 | Train Loss 0.00012738375786624078\n",
      "\n",
      "Epoch 87 | Test DotSimilarity is 0.8548667323150803\n",
      "Epoch 87 | Validation DotSimilarity is 0.8557191723648151\n",
      "Epoch 87 | Train Loss 0.00012621730290575365\n",
      "\n",
      "Epoch 88 | Test DotSimilarity is 0.8556083337308753\n",
      "Epoch 88 | Validation DotSimilarity is 0.8573417890862842\n",
      "Epoch 88 | Train Loss 0.00012541449282779012\n",
      "\n",
      "Epoch 89 | Test DotSimilarity is 0.8533282972090759\n",
      "Epoch 89 | Validation DotSimilarity is 0.8546365393008866\n",
      "Epoch 89 | Train Loss 0.00012491887840547982\n",
      "\n",
      "Epoch 90 | Test DotSimilarity is 0.8548571891304888\n",
      "Epoch 90 | Validation DotSimilarity is 0.8566504564184175\n",
      "Epoch 90 | Train Loss 0.00012459310968100776\n",
      "\n",
      "Epoch 91 | Test DotSimilarity is 0.8554058826635579\n",
      "Epoch 91 | Validation DotSimilarity is 0.8568137711632356\n",
      "Epoch 91 | Train Loss 0.00012335460835322237\n",
      "\n",
      "Epoch 92 | Test DotSimilarity is 0.8556413071723431\n",
      "Epoch 92 | Validation DotSimilarity is 0.8565969198252015\n",
      "Epoch 92 | Train Loss 0.00012300339297958605\n",
      "\n",
      "Epoch 93 | Test DotSimilarity is 0.8555618075951985\n",
      "Epoch 93 | Validation DotSimilarity is 0.8573060955247492\n",
      "Epoch 93 | Train Loss 0.00012271906371269745\n",
      "\n",
      "Epoch 94 | Test DotSimilarity is 0.8558936151109822\n",
      "Epoch 94 | Validation DotSimilarity is 0.8577311133261177\n",
      "Epoch 94 | Train Loss 0.00012141525324154968\n",
      "\n",
      "Epoch 95 | Test DotSimilarity is 0.8556859401370608\n",
      "Epoch 95 | Validation DotSimilarity is 0.8573160013866146\n",
      "Epoch 95 | Train Loss 0.00012134432183788984\n",
      "\n",
      "Epoch 96 | Test DotSimilarity is 0.8556637501064023\n",
      "Epoch 96 | Validation DotSimilarity is 0.856300537129767\n",
      "Epoch 96 | Train Loss 0.00012044892982294569\n",
      "\n",
      "Epoch 97 | Test DotSimilarity is 0.8553672917536532\n",
      "Epoch 97 | Validation DotSimilarity is 0.857347105919645\n",
      "Epoch 97 | Train Loss 0.00012031923525528423\n",
      "\n",
      "Epoch 98 | Test DotSimilarity is 0.8544607742575606\n",
      "Epoch 98 | Validation DotSimilarity is 0.8556743089777319\n",
      "Epoch 98 | Train Loss 0.00012188995979015351\n",
      "\n",
      "Epoch 99 | Test DotSimilarity is 0.8570886741222795\n",
      "Epoch 99 | Validation DotSimilarity is 0.8584194547346436\n",
      "Epoch 99 | Train Loss 0.00011316257465760126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "######################################\n",
    "#  LOSS\n",
    "######################################\n",
    "\n",
    "loss_fn = torch.nn.HuberLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005) \n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "\n",
    "# Use GPU for training\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap data in a data loader\n",
    "\n",
    "\n",
    "NUM_GRAPHS_PER_BATCH = 64\n",
    "\n",
    "\n",
    "loader = DataLoader(train_dataset, \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "SAVE_EVERY_X_EPOCH = 10\n",
    "REPORT_EVERY_X_EPOCH = 1\n",
    "\n",
    "def train(loader):\n",
    "    # Enumerate over the data\n",
    "    loss_per_batch = np.array([])\n",
    "    for batch in loader:\n",
    "        # Use GPU\n",
    "        batch.to(device)  \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Passing the node features and the connection info\n",
    "        pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch) \n",
    "        # Calculating the loss and gradients\n",
    "        loss = loss_fn(pred, batch.y)\n",
    "        loss.backward()  \n",
    "        # Update using the gradients\n",
    "        optimizer.step()\n",
    "        loss_per_batch = np.concatenate((loss_per_batch, np.array([loss.clone().detach().cpu().numpy()])))\n",
    "    return loss_per_batch\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(300):\n",
    "    scheduler.step()\n",
    "    loss = train(loader)\n",
    "    pred_test_similarity = np.array([])\n",
    "    pred_validation_similarity = np.array([])\n",
    "   \n",
    "   \n",
    "    if epoch % REPORT_EVERY_X_EPOCH == 0:\n",
    "        for batch in test_loader:\n",
    "            batch.to(device)  \n",
    "            pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch)\n",
    "\n",
    "            batch_similarity = validate_similarities(batch.y.detach().cpu().numpy(),\n",
    "                                  pred.detach().cpu().numpy(),\n",
    "                                  mass_pow=1.0, intensity_pow=0.5)\n",
    "\n",
    "            pred_test_similarity = np.concatenate((pred_test_similarity, batch_similarity))\n",
    "    \n",
    "        for batch in validation_loader:\n",
    "           \n",
    "            batch.to(device)  \n",
    "            pred = model(batch.x.float(), batch.edge_index, batch.edge_attr, batch.molecular_weight, batch.batch)\n",
    "\n",
    "            batch_similarity = validate_similarities(batch.y.detach().cpu().numpy(),\n",
    "                                  pred.detach().cpu().numpy(),\n",
    "                                  mass_pow=1.0, intensity_pow=0.5)\n",
    "\n",
    "            pred_validation_similarity = np.concatenate((pred_validation_similarity, batch_similarity))\n",
    "            \n",
    "        \n",
    "        print(f\"Epoch {epoch} | Test DotSimilarity is {pred_test_similarity.mean()}\")\n",
    "        print(f\"Epoch {epoch} | Validation DotSimilarity is {pred_validation_similarity.mean()}\")\n",
    "        print(f\"Epoch {epoch} | Train Loss {loss.mean()}\")\n",
    "        print()\n",
    "\n",
    "    \n",
    "    if epoch % SAVE_EVERY_X_EPOCH == 0:\n",
    "        SAVE_PATH = f\"{epoch}.pt\"\n",
    "            \n",
    "        # Save model\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'metadata': {\"loss\" : \"HuberLoss\",\n",
    "                     \"Dataset\": \"Preprocessed_test_pow_preparation_no_sparse_small\",\n",
    "                     \"test_similarities\": pred_test_similarity.mean()}\n",
    "        }, os.path.join(MODEL_SAVE, SAVE_PATH))\n",
    "\n",
    "        LOSS_FILE = f\"all_loss_until_{epoch}.output\"\n",
    "        with open(os.path.join(MODEL_SAVE, LOSS_FILE), 'wb') as fid:\n",
    "            pickle.dump(loss.mean(), fid)\n",
    "            fid.close() \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
